# Time Domain Audio Visual Speech Separation

## Abstract

视听多模态建模已被证明在许多与语音相关的任务中是有效的,例如语音识别和语音增强。本文介绍了一种新的时域视听架构,用于从单声道混合物中提取目标说话人。该架构通用化了以前的TasNet（时域语音分离网络）以实现多模态学习,同时将经典的视听语音分离从频域扩展到了时域。所提出的体系结构的主要组件包括音频编码器,从视频流中提取嘴唇嵌入的视频编码器,多模态分离网络和音频解码器。根据最近发布的LRS2数据集进行的模拟混音实验表明,与仅使用音频的TasNet和频域视听网络相比,我们的方法可以分别在2个和3个说话人的情况下带来3dB和4dB的Si-SNR改善。

## 1. Introduction

语音分离的目标是将每个源说话人从混合信号中分离出来。尽管已经研究了很多年,但是语音分离仍然是一个难题,尤其是在嘈杂和回响的环境中。最近提出了几种纯音频的语音分离方法,例如uPIT [1],DPCL [2,3],DANet [4]和TasNet [5]。但是,在这些方法中,目标说话人的数量必须作为先验信息,并且在训练和测试期间保持不变。另外,这些系统的分离结果不能与说话人相关联,这极大地限制了它们的应用场景。

如果能够提取一些目标说话人相关特征,那么语音分离的任务将成为目标说话人提取问题。与盲分离方法相比,这具有几个明显的优势。首先,由于该模型每次仅从混合物中提取一位目标说话人,因此不再需要有关说话人数量的先验知识。其次,由于将目标说话人特征赋予模型,显然避免了标签排列的问题。这些使得目标说话人的分离比盲分离解决方案更实用。

过去已经探索了几种目标说话人分离方法[6,7,8]。在[8]中,作者提出了一个名为VoiceFilter的系统,该系统使用d矢量[9]作为分离网络目标说话人的embedding。同样[7]使用短的锚点句子作为目标说话人分离的辅助输入。但是,基于说话人或句子的特征不够鲁棒,可能会严重影响系统性能,尤其是在存在噪声或混有同性别说话人的情况下。

另一个可供选择的,视觉信息是对噪声不敏感的并且与语音内容高度相关。先前已经研究了将音频和视觉信息结合起来以进行自动语音识别（ASR）[10,11,12],语音增强[13,14]和语音分离[15,16]。结果显示出利用视觉特征作为补充信息源的巨大潜力。对于语音分离,[13]提出了一种编码器-解码器模型体系结构,以将可见说话人的声音与背景噪声分开。带噪语音频谱图和中心裁剪视频帧分别用作音频/视频编码器的输入。[15]通过堆叠深度可分离的卷积块构建了一个更深的网络,其中包括一个幅值网络来估计时频（TF）掩码[17],以及一个相网络来从混合相位中预测干净相位。幅度网络使用预训练的唇部embedding[18]作为视觉特征。同样[16]建立了一个基于膨胀卷积和LSTM的模型,该模型使用面部embedding作为视觉特征,并在大规模视听数据集上产生了良好的效果。

多数以前的视听分离系统已经在TF域中处理了音频流，因此，估计的TF mask的准确性是这些系统成功的关键。另一方面，虽然可以显着影响分离质量，但对信号相位的考虑却较少[19，20，21，22]。为了合并相位信息，[15]使用了相位子网来细化原始的带噪相位，[16]采用了新提出的复数mask[20]而不是传统的实值mask（例如IRM [1]，PSM [23]等等）。最近，[5，24]提出了一个名为TasNet的新的编解码器框架，该框架直接在时域上分离语音，并在公共WSJ0-2mix数据集上产生令人印象深刻的结果。在本文中，我们提出了一个新的分离网络，该网络可以概括Tas-Net，以实现听觉和视觉信号的多模式融合。给定混合语音的原始波形和目标说话人的相应视频流，我们的视听语音分离模型可以直接提取目标说话人的音频。

这项工作的贡献包括：

1. 提出了一种用于多模态语音分离的新结构，并且为了说明该结构的有效性，对三种典型的分离模型uPIT [1]（频域音频），Conv-TasNet [5]（仅时域音频）和Conv-FavsNet（频域视听）进行了全面比较。
2. 据我们所知，这是直接在时域上执行视听分离的第一部作品。最近发布的n-the-wild视频[11]的实验表明，与所有其他基准模型相比，该结构带来了明显的改进。
3. 先前的视觉特征对语音分离来说设计不佳。在这项工作中，专门训练视觉（嘴唇）embedding来表示语音信息。还研究了嵌入网络的不同建模单元，例如单词，音素（CI音素）和上下文相关音素（CD音素），以及比较。

## 2. PROPOSED SYSTEM

本节将介绍我们提出的时域视听语音分离网络的体系结构。总的来说，网络将被馈入大量原始波形和相应的视频帧，并直接预测目标说话人的语音，如图1所示。标度不变的信噪比（Si-SNR）为用作训练目标函数[5]，其定义为:
$$
\mathrm{Si-SNR}=20 \log _{10} \frac{\|\alpha \cdot \mathbf{s}_{t}\|}{\|\mathbf{s}_{e}-\alpha \cdot \mathbf{s}_{t}\|} \tag{1}
$$
其中，$\mathbf{s}_{e}$和$\mathbf{s}_{t}$分别是估计的信号和目标源，他们都被规整到0均值，$\alpha$是一个最佳的尺度放缩因子，计算式为:
$$
\alpha=\mathbf{s}_{e}^{T} \mathbf{s}_{t} / \mathbf{s}_{t}^{T} \mathbf{s}_{t}
$$
我们还使用Si-SNR作为评估指标，与原始SDR相比，它被认为是更可靠的分离质量衡量指标[25]。

### 2.1. Overview

所提出的结构主要是受[24]中提出的TasNet结构的启发，该结构包含三个部分，一个音频编码器/解码器和一个分离网络。给定一个音频混合块$\mathrm{x}=\{x_0,x_1,\cdots,x_C\}$($C$表示块大小)，TasNet中的音频编码器会尝试将混合物采样编码为一些非负矢量序列$\mathrm{w}_x$，而这些分离网络会估计在这样定义的空间的每个源的mask。音频解码器用于将每个mask的结果再次重构到时域。整个框架可以描述为:
$$
\mathrm{s}_i=\text{Decoder}(\mathrm{w}_x\odot\mathrm{m}_i) \tag{3}
$$
其中，
$$
\begin{aligned}
\mathbf{w}_{x} &=\text { Encoder }^{a}(\mathbf{x}) \\
[\mathbf{m}_{0}, \cdots, \mathbf{m}_{N-1}] &=\operatorname{Separator}(\mathbf{w}_{x})
\end{aligned} \tag{4}
$$
$\odot$表示Hadamard乘积。$N$是混合物中的说话人源数目。$\mathbf{m}_i$和$\mathbf{s}_i$分别代表第$i$个说话人的估计mask和分离结果。

当视频流可用时，我们的动机是通过提供音频表示$\mathbf{w}_x$和视频编码特征$\mathbf{v}_e$来偏置分离网络。因此，分离器仅生成目标说话人的mask，可以将其写为:
$$
\begin{aligned}
\mathbf{v}_{e} &=\text{Encoder}^{v}(\mathbf{v}_{t}) \\
\mathbf{m}_{t} &=\operatorname{Separator}(\mathbf{w}_{x}, \mathbf{v}_{e})
\end{aligned} \tag{5}
$$
$\mathbf{v}_t$表示目标说话人的图像帧序列，而$\text{Encoder}^{a/v}$表示音频/视频编码器。最后，可以通过以下公式获得单独的结果：
$$
\mathbf{s}_t=\operatorname{Decoder}(\mathbf{w}_x\odot\mathbf{m}_t) \tag{6}
$$

### 2.2. Video encoder

视频编码器旨在从输入图像帧序列中提取视觉特征。在我们的实验中，我们仅使用嘴唇区域，因为它对我们需要的上下文和音素信息进行编码。通常，视频编码器包含一个嘴唇embedding提取器，然后是几个时序卷积块，如图1所示。

Figure1. 建议的时域视听分离网络。在输入时空卷积块Conv3D之前，每个输入图像帧的嘴唇区域$\mathbf{v}_t$都被裁剪并调整为112×112的大小[18]。嘴唇embedding$\mathbf{l}_e$，音频编码器输出$\mathbf{w}_x$，视频编码器输出$\mathbf{v}_e$，音频编码特征$\mathbf{a}_e$和融合特征$\mathbf{f}$都是256维的序列。

唇部embedding提取器由3D卷积层和18层ResNet组成[26]，类似于[15]中的工作。提取器为每个视频帧输出固定维特征向量，并通过随后的时间卷积块。每个块由时间卷积组成，之后是ReLU激活和bn[27]。残差连接也包括在内，尽管根据我们的结果它们不会产生重大影响。为了减少模型参数，我们使用深度可分离的卷积[28]。在我们的设置中，我们使用256维唇形embedding，并为所有块选择核大小为3，步长为1和通道数为512。

按照[18]中介绍的步骤，对唇部embedding提取器进行单独的特定后端预训练。之后，将提取器固定。除了在[15]中使用的词级分类目标之外，我们还从ASR的角度尝试音素（CI-phone）和上下文相关的音素（CD-phone），以改善唇部embedding的质量。

### 2.3. Audio encoder/decoder

音频编码器和解码器分别对混合音频信号$\mathbf{x}$和masked的编码序列执行一维卷积和解卷积操作。它们可以表示为：
$$
\begin{array}{l}
\text { Encoder }^{a}(\mathbf{x})=\operatorname{ReLU}(\operatorname{conv}_{1 \mathrm{D}}(\mathbf{x}, K, S)) \\
\text { Decoder }(\mathbf{x})=\operatorname{deconv}_{1 \mathrm{D}}(\mathbf{x}, K, S)
\end{array} \tag{7}
$$
$K$和$S$分别表示一维卷积运算中的内核大小和步长。在本文中，我们默认使用$K=40$和$S=20$。

### 2.4. Separation network

分离网络旨在根据编码的音频和视觉特征来估计目标说话人的mask。它由几个时间膨胀的卷积块堆叠在一起，并提供了一种简单的机制来处理特征融合和同步。分离网络中一维扩散卷积块的结构类似于Conv-TasNet中使用的结构。在本文中，我们将其表示为$Conv_s$。每个$Conv_s$有$D$个具有指数增长的膨胀因子$2^d$子块，其中$d\in\{0,\ldots,D-1\}$，如图1所示。在我们的设置中，我们使用$D=8$。

音频编码器的输出$\mathbf{w}_x$首先通过$N_a$个卷积块:
$$
\mathbf{a}_{e}=\overbrace{\operatorname{Conv}_{s}(\cdots \operatorname{Conv}_{s}(\mathbf{w}_{x}))}^{N_{a}} \tag{8}
$$
然后与视觉特征$\mathbf{v}_e$融合。融合过程是通过在卷积通道维度上进行简单的串联操作来执行的，然后进行位置投影$\mathcal{P}$以减小特征尺寸。为了同步音频和视频功能的时间分辨率，必要时对视频流进行上采样。上面的描述可以写成：
$$
\mathbf{f}=\mathcal{P}([\mathbf{a}_{e} ; \text { Upsample }(\mathbf{v}_{e})]) \tag{9}
$$
最后，融合特征$\mathbf{f}$通过$N_f$个卷积块估计目标mask $\mathbf{m}_t$：
$$
\mathbf{m}_{t}=\sigma(\overbrace{\operatorname{Conv}_{s}(\cdots \operatorname{Conv}_{s}(\mathbf{w}_{x}))}^{N_{f}}) \tag{10}
$$
$\sigma$表示任意非线性函数。我们在实验中使用$\operatorname{ReLU}$，而不会失去一般性。

## 3. EXPERIMENTS AND RESULTS

### 3.1. Dataset

在实验中，我们使用 Oxford-BBC Lip Reading Sentences 2（LRS2）数据集的句子创建了两个说话人和三个说话人的混合物，其中包括来自BBC电视台的数千个口头句子及其相应的转录。训练，验证和测试集是根据广播日期生成的，因此这些集不会重叠。

丢弃短句子（少于2s），总共使用21075句语音来生成数据，其中19445进行了训练，其余的分别用于验证和测试。表1中总结了用于仿真的语音细节。通过随机选择不同的句子并选择-5 dB和5dB之间的各种信噪比（SNR）来混合它们以生成二、三说话人混合。采样率为16kHz。为确保每种资源的视频都可混合使用，较长的资源将被截断以与最短的资源对齐。源片段与25fps的视频流同步。最后，我们分别模拟了40k（总共25h），5k和3k语音用于训练，验证和测试集。

### 3.2. Training details

类似于[15]，我们首先在LRW数据集上训练嘴唇embedding提取器。这是一个词级分类任务，我们在测试集上实现了76.02％的分类精度[18]。但是，LRW数据集没有提供句子级音频转录，这使得用较小的片段替换词级训练目标不可行，例如，上面提到的CD/CI音素。相反，我们选择LRS2的预训练集来训练音素级嘴唇embedding提取器。

这些对齐方式是根据Kaldi [29]的脚本从GMM声学模型得出的，并在训练之前被子采样到视频采样率。我们从CMU词典中为CI音素选择44个单元，并从为CD音素设置的路线中获得3048个单元。视频帧被转换为灰度并相对于全局均值和方差归一化。训练进度与词级任务相似，不同之处在于使用帧级交叉熵损失而不是序列级。

视听网络使用Adam [30]优化器和2s音频/视频块进行8个epoch训练，并在6个epoch的验证损失没有改善的情况下提前停止。初始学习率设置为1e-3，如果在3个epoch内验证损失没有改善，则在训练期间将其减半。

### 3.3. Results and comparisons

在这两个数据集上，我们首先报告两个典型的理想掩码以及两个基于音频的常规方法的结果：分别在频域和时域上的uPIT-BLSTM和Conv-TasNet [5]。理想掩码的结果是相应的基于掩码的方法的上限。uPIT-BLSTM采用三层Bi-LSTM结构，PSM（相敏掩码[23]）用作训练目标。对于Conv-TasNet，我们选择$K=40,S=20$的音频编码器/解码器，并在分离网络中选择较大的瓶颈大小（384），以带来更好的性能。结果如表2所示。值得一提的是，LRS2中的音频流不如WSJ0干净，因此，这些纯音频模型的报告结果比WSJ0-2mix的先前结果差，尤其是当说话人数量增加时。

#### 3.3.1. Results with different lip embeddings

在表3中，我们通过在LRW数据集上训练的词级嘴唇嵌入来评估提议的视听模型，与表2中的纯音频方法相比，这已经带来了显着的改进。由于LRS2数据集上的视频帧的分辨率与LRW不匹配时，我们裁剪图像中心70×70像素区域，然后将其重新采样为112×112。用CI音素替换单词级目标可带来更好的结果，尤其是在困难的任务上，即表3所示的三说话人测试集。

Norm技术在我们的实验中也很关键。在表3中，用gLN替换批次BN可以在两个和三个说话人的混合数据集上得到提高。在我们的初步实验中，视频块中的残差连接对最终结果的影响较小。而且，通过增加视频编码器中的块数，可能没有获得明显的改善，这可能是由于训练有素的视觉特征所致。

#### 3.3.2. Impact of separation networks

最终结果很大程度上取决于分离网络产生的目标说话人掩模。除了3.3.1节中提到的标准化技术外，我们还调整了分离网络的$N_a$和$N_f$参数。

通过固定块总数（本工作中使用4个块），增加$N_f$的值可提供更多融合特征的上下文信息。表4说明了使用$N_a=1$和$N_f=3$可获得最佳性能。

基于以上讨论，我们进一步比较了三说话人数据集上使用CD/CI音素，具有不同$N_a$和$N_f$对。结果表明，使用CD音素作为训练目标虽然效果更好，但效果稍差。因此，我们在以下实验中选择CI音素。

### 3.3.3. Comparision with the frequency-domain networks

为了进一步研究时域视听方法的进步，我们在相同数据集上训练了具有相似参数大小的频域视听分离模型，以进行比较。在本文中，我们称其为Conv-FavsNet。Conv-FavsNet从提议的体系结构中删除了音频编码器，并用线性层替换了解码器，该层将卷积块的输出转换为TF掩码。我们使用以40 ms hanning窗口和10 ms位移计算的线性频谱图作为输入音频特征并以PSM作为训练目标。相敏频谱近似（PSA）的损耗函数定义为：
$$
\mathcal{L}=\|\mathbf{s}_{t} \odot \max \{\cos (\angle \mathbf{s}_{m}-\angle \mathbf{s}_{t}), 0\}-\mathbf{s}_{m} \odot \mathbf{m}_{t}\|_{2}^{2} \tag{11}
$$
其中$\mathbf{m}_t$表示估计的目标说话人mask，$\mathbf{s}_t$，$\mathbf{s}_m$分别表示目标说话人和混合信号的幅度。结果如表5所示。iSTFT中使用的噪声相位肯定会影响重构波形的质量，因为理想相位可以带来3dB Si-SNR的额外改善，从而与我们的时域视听结果相匹配。

### 3.3.4. Multi-speaker training

对于有偏见/知情的分离系统，我们发现经过艰巨任务训练的模型可以很好地完成简单任务。例如，在我们的实验中，在三说话人数据集上训练的模型在两说话人混合信号上也表现良好。这激发了我们训练混合两个和三个说话人的模型的方法，这在[3]中被称为多说话人训练。

实际上，目标分离网络的体系结构与说话人的数量无关。训练样本的标签由辅助特征确定，例如这项工作中的唇部embedding。这适用于现实情况，因为大多数情况下很难检测到说话人的人数。表6显示了仅在2个说话人混合，仅在3个说话人混合上训练以及使用多说话人训练的模型的性能。在3个说话人混合训练下的模型可以推广到2个说话人的场景，但比两说话人模型的性能差一点。表6中的所有设置都对多说话人训练集进行了多说话人训练的改进。具有多说话人训练的最佳配置在两个和三个说话人测试集上分别达到了14.02dB和9.92dB的Si-SNR。

## 4. CONCLUSIONS

在本文中，我们提出了一种时域视听目标语音分离架构，该架构结合了原始波形和目标说话人的视频流，可以直接预测目标语音波形。嘴唇embedding提取器经过预训练，可以从视频流中提取运动信息。我们发现，单词级和音素级嘴唇embedding有效地有益于分离网络。与纯音频方法和频域视听方法相比，该方法在两个和三个说话人测试集的Si-SNR方面分别提高了3dB和4dB以上。

Table1. 用于模拟2个和3个说话人混合的每个数据集的句子数

Table2. 理想的TF-mask和纯音频方法在2和3说话人测试集上的结果

Table3. 具有不同嘴唇embedding的模型的结果

Table4. 融合块数量调整的结果

Table5. 频域视听模型的结果

Table6. 经过多说话者训练的模型的结果
