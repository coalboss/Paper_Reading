# Contextual Audio-Visual Switching For SpeechEnhancement in Real-World Environments



## Abstract

人类语音处理本质上是多模态的，其中视觉提示（嘴唇移动）有助于更好地理解噪声中的语音。唇读驱动的语音增强在低信噪比（SNR）方面明显优于基准音频方法。但是，在高SNR或低水平的背景噪声的情况下，视觉提示在语音增强方面的效果相当差，并且仅音频提示可以正常工作。因此，需要一种更优化的上下文感知视听（AV）系统，该系统在上下文中同时利用视觉和嘈杂的音频功能，并有效地解决了各种嘈杂的情况。在本文中，我们介绍了一种新颖的上下文AV切换组件，该组件根据不同的操作条件在上下文中利用AV提示来估计干净的音频，而无需任何SNR估计。SNR分别低，高和中级别时，切换模块分别处于仅视觉（仅V），仅音频（仅A）和视听。通过集成卷积神经网络（CNN）和长短期记忆（LSTM）网络来开发上下文AV切换组件。为了进行测试，开发的新型增强型视觉驱动的维纳滤波器（EVWF）将估计的干净音频功能用于干净的音频功率谱估计。使用基准Grid和ChiME3语料库，在不同SNR级别（从低到高SNR）的动态现实世界场景（咖啡馆，街道，BUS，行人）下评估了音视频AV语音增强方法。对于客观测试，语音质量的感知评估（PESQ）用于评估恢复的语音的质量。对于主观测试，使用标准均值评分（MOS）方法。批判性分析和比较研究表明，在低信噪比和高信噪比下，提出的上下文AV方法比基于A-Only，V-Only，频谱减法（SS）和对数最小二乘平方误差（LMMSE）的语音增强方法的性能更高，揭示了其在任何现实世界的嘈杂条件下应对频谱时变的能力。仿真结果还揭示了在高SNR时视觉提示效果较差的现象，在低SNR时音频提示效果较差的现象以及互补的音频和视觉提示强度的现象。最后，使用彩色频谱图展示了以低SNR使用视觉提示的好处，其中与仅使用A的提示相比，视觉提示可以更好地恢复特定时频单元的语音成分。

## 1. Introduction

通过看着说话者的嘴唇来部分理解语音的能力被称为“唇读”。正常人和听力受损的人都可以通过唇读来改善听力。在嘈杂的环境中，特别是当将仅音频的感知与AV感知进行比较时，它可以提高语音清晰度。嘴读会带来好处，因为可见的咬合器，例如嘴唇，牙齿和舌头与声道的共振相关。先前在众多行为研究中已经显示了发音器官的可见特性与语音接收之间的相关性[1] [2] [3] [4]。诸如[5] [6]的早期研究表明，唇读可以帮助容忍额外的4-6 dB的噪声，其中1分贝的SNR增益相当于可懂度提高了10-15％。在高噪声环境（以及混响）中，中高频的听觉表现通常会降低[2]。在这种情况下，如果听力障碍的人要像正常听力的听众一样准确地感知语音，则每10 dB的听力损失，SNR就要提高约1 dB[7]。

在文献中，已经进行了广泛的研究来开发多模态语音处理方法，这确立了多模态信息在语音处理中的重要性[8]。研究人员提出了新颖的视觉特征提取方法[9] [10] [11] [12] [13]，融合方法（早期整合[14]，后期整合[15]，混合整合[16]），多模式数据集[17] [18]和融合技术[16] [19] [20]。多模式视听语音处理方法在ASR，语音增强和语音分离方面已显示出显着的性能提升[21] [22] [23] [24] [25]。最近，[26]中的作者开发了一种视听深度CNN（AVDCNN）语音增强模型，该模型将音频和视觉提示集成到统一的网络模型中。提出的AVDCNN方法被构造为视听编码器-解码器，其中使用两个单独的CNN来处理视听线索，然后融合到联合网络中以产生增强的语音。但是，提出的AVDCNN方法仅依赖于深层CNN模型，为了进行测试，作者使用了自行准备的数据集，其中包含由母语使用者（仅一名说话者）说出的320句普通话的视频记录。通常的汽车引擎，婴儿哭泣，纯音乐，带歌词的音乐，警报器，一个背景通话人（1T），两个背景通话人（2T）和三个背景通话人（3T）。总而言之，为了测试，只考虑了一个说话人（在40句干净的语音与10种不同类型的噪声以5 dB，0 dB和-5 dB SIR混合时）。

与[26]相反，提出的语音增强框架利用了深度学习和分析声学建模（基于滤波的方法）的互补优势。另外，所引入的上下文AV组件通过在上下文中利用视觉和噪声音频特征来有效地解决了不同的噪声条件。可以相信，我们的大脑通过在上下文中利用和整合多模式提示（如唇读，面部表情，肢体语言，手势，音频和文本信息）以相同的方式工作，以进一步提高语音质量和清晰度。但是，为在嘈杂的场景中进行语音增强应用而在上下文中利用多模式AV切换进行的工作有限。

Figure1：用于语音增强的上下文视听切换。该系统根据上下文利用音频和视觉提示来估计干净的音频功能。然后，将估计的干净音频特征输入到建议的增强的视觉驱动的维纳滤波器中，以估计干净的语音频谱。

Figure2：上下文视听切换模块：CLSTM和LSTM与5个先前音频和视觉帧的集成（考虑到当前的AV帧$t_k$和先前5个AV帧的时间信息$t_{k-1},t_{k-2},t_{k-3},t_{k-4},t_{k-5}$）

本文的其余部分安排如下：第2节介绍了提出的语音增强框架，包括上下文视听切换和EVWF。第3节介绍了所采用的AV数据集和视听特征提取方法。在第4节中，提供了对比实验结果。最后，第5节总结了这项工作以及一些未来的研究方向。

## 2. Speech Enhancement Framework

图1中描述了所提出的上下文感知语音增强框架的示意性框图。所提出的体系结构包括音频和视频特征提取模块，上下文感知的AV切换组件，语音过滤组件以及波形重构模块。上下文AV切换组件同时利用音频和视觉功能，以了解不同带噪情况下输入（嘈杂的音频和视觉功能）与输出（清晰语音）之间的相关性。设计的上下文感知AV切换组件由两个深度学习体系结构LSTM和CLSTM组成。然后，将LSTM和CLSTM驱动的估计的干净音频特征输入到设计的EVWF中，以计算出最佳的维纳滤波器，然后将其应用于有噪声的输入音频信号的幅度谱，然后进行波形重构过程。第2.1节和第2.2节全面介绍了更多详细信息。

### 2.1. Contextual Audio-Visual switching

提出的深度学习驱动的上下文感知AV切换架构在图2中进行了描述，该架构利用了CNN和LSTM的互补优势。上下文感知一词表示提出的AV切换组件的学习能力和适应能力，它有助于根据上下文利用多模态提示，以更好地应对所有可能的语音场景（例如，低，中，高背景噪音，不同的环境（家庭，餐厅，社交聚会等））。用于视觉提示处理的CLSTM网络包括输入层，四个卷积层（分别具有大小为3×5的16、32、64和128个滤波器），四个大小为2×2的max pooling层以及一个LSTM层（100个单元）。用于音频提示处理的LSTM网络包括两个LSTM层。将时间实例$t_k,t_{k-1},\ldots,t_{k-5}$（与当前时间实例相同，而5是先前视觉帧的数量）的视觉特征输入到CLSTM模型中，以提取优化特征，然后由2个LSTM层加以利用。时间实例$t_k,t_{k-1},\ldots,t_{k-5}$的音频特征被输入到具有两个LSTM层的LSTM模型中。第一LSTM层有250个单元，对输入进行编码，并将其隐藏状态传递给第二个LSTM层，该第二层具有300个单元。最后，使用两个dense层融合了CLSTM和LSTM的优化潜在特征。对该体系结构进行了训练，其目标是使预测的和实际的音频feature之间的均方误差(MSE)最小化。使用随机梯度下降算法和RMSProp优化器将估计的音频logFB特征与纯净音频特征之间的MSE（1）最小化。RMSprop是一种自适应学习率优化器，它通过移动最近梯度幅度的平均值来划分学习率，从而使学习更加有效。此外，为了减少过度拟合，在每个LSTM层之后都应用了dropout（0.20）。MSE成本函数$C(a_{estimated}, a_{clean})$可以写成：
$$
C(a_{\text {estimated }}, a_{\text {clean }})=\sum_{i=1}^{n} 0.5(a_{\text {estimated }}(i)-a_{\text {clean }}(i))^{2}
$$
其中，$a_{estimated}, a_{clean}$分别是估计的和干净的语音特征。

### 2.2 Enhanced Visually Derived Wiener Filter

EVWF使用可逆滤波器组（FB）变换将估计的低维纯净音频特征转换为高维纯净音频功率谱，并计算维纳滤波器。然后将维纳滤波器应用于带噪的输入音频信号的幅度谱，然后进行快速傅立叶逆变换（IFFT），重叠和合并过程以产生增强的幅度谱。最新的VWF和设计的EVWF分别在图3（a）和（b）中描述。[27]中的作者提出了一种基于隐马尔可夫模型-高斯混合模型（HMM/GMM）的两级最新VWF，用于语音增强。然而，将HMM/GMM和三次样条插值用于干净的音频特征估计和低到高维变换不是最佳选择，这是由于HMM/GMM模型的泛化能力有限以及三次样条插值的音频功率谱估计很差无法估计缺少的功率谱值的方法。相反，提出的EVWF利用基于LSTM的FB估计和反向FB变换进行音频功率谱估计，从而解决了VWF中的功率谱估计和泛化问题。此外，提出的EVWF取代了语音活动检测器和噪声估计的需求。

频域维纳滤波器定义为：
$$
W(\gamma)=\frac{\psi_{\hat{a}}(\gamma)}{\psi_{a}(\gamma)} \tag{2}
$$
其中$\psi_{a}(\gamma)$是带噪音频功率谱（即干净功率谱+噪声功率谱），而$\psi_{\hat{a}}(\gamma)$是干净音频功率谱。由于可用的带噪语音频矢量，因此带噪语音功率谱的计算非常容易。然而，干净音频功率谱的计算是有挑战性的，这限制了维纳滤波器的广泛使用。因此，为了成功进行维纳滤波，必须获取干净的音频功率谱。在本文中，使用基于深度学习的唇读驱动语音模型来计算干净音频功率谱。

FB域维纳滤波器$(\hat{W}_t^{FB}(k))$在[27]中被给出：
$$
\hat{W}_t^{FB}(k)=\frac{\hat{x}_t(k)}{\hat{x}_t(k)+\hat{n}_t(k)} \tag{3}
$$
其中$\hat{x}_t(k)$是FB域唇读驱动的近似纯音频特征，而$\hat{n}_t(k)$是FB域噪声信号。下标$k$和$t$代表第$k$个通道和第$t$个音频帧。

唇读驱动的近似干净音频特征矢量$\hat{x}_t(k)$是一个低维矢量。为了高维维纳滤波器的计算，需要将估计的低维FB域音频系数转换为高维功率谱域。要注意的是，近似的干净音频和噪声特征$(\hat{x}_t(k)+\hat{n}_t(k))$被噪声音频（真实干净语音$x_t(k)$和噪声$n_t(k)$的组合）代替。低维到高维的转换可以写成:
$$
\hat{W}_{t_{[N_{l}, M]}}^{F B}(k)=\frac{\hat{x}_{t(k)_{[N_{l}, M]}}}{x_{t(k)_{[N_{l}, M]}}+n_{t(k)_{[N_{l}, M]}}} \tag{4}
$$
$$
\hat{W}_{t_{[N_{h}, M]}}^{F B}(k)=\frac{\hat{x}_{t(k)_{[N_{h}, M]}}}{x_{t(k)_{[N_{h}, M]}}+n_{t(k)_{[N_{h}, M]}}} \tag{5}
$$
其中$N_l$和$N_h$分别具有低维和高维音频特征，而$M$是音频帧数。从（4）到（5）的转换对于滤波性能至关重要。因此，[27]中的作者提出使用三次样条插值方法来确定丢失的光谱值。相比之下，本文建议使用FB逆变换，其计算如下：
$$
\hat{x}_{t(k)_{[N_{h}, M]}}=\hat{x}_{t(k)_{[N_{l}, M]}} * \alpha_{x} \tag{6}
$$
$$
n_{t(k)_{[N_{h}, M]}}=n_{t(k)_{[N_{l}, M]}} * \alpha_{n} \tag{7}
$$
$$
\alpha_{x}=\alpha_{n}=(\phi_{m}(k)^{T} \phi_{m}(k))^{-1} \phi_{m}(k)^{T} \tag{8}
$$
$$
\phi_m(k)=\left\{
\begin{aligned}
& 0 & k\lt f_{b_{mf-1}} \\
& \frac{k-f_{b_{mf-1}}}{f_{b_{mf}}-f_{b_{mf-1}}} & f_{b_{mf-1}}\le k \le f_{b_{mf}}\\
& \frac{f_{b_{mf-1}}-k}{f_{b_{mf+1}}+f_{b_{mf}}} & f_{b_{mf}}\le k \le f_{b_{mf+1}} \\
& 0 & k\gt f_{b_{mf}}
\end{aligned}
\right.
$$
其中$f_{b_{mf}}$是滤波器的边界点，并且对应于$k$点DFT的第$k$个系数。使用以下公式计算边界点：
$$
f_{b_{mf}}=(\frac{K}{F_{sample}}\cdot f_{cm_f}) \tag{9}
$$
其中$f_{cm_f}$是梅尔频率。

替换后，将获得的第$k$单元维纳滤波器（5）应用于带噪音频信号$(|Y_t(k)|)$的频谱，以估计增强的幅度音频频谱$(|\hat{X}_t(k)_{[N_H,M]}|)$。增强幅度音频频谱为：
$$
|\hat{X}_t(k)|=|Y_t(k)||\hat{W}_{t_{[N_h,M]}}| \tag{10}
$$
然后，在获取的时域增强语音信号（10）之后进行IFFT，重叠和合并过程。

## 3. AudioVisual Corpus & Feature Extraction

在本节中，我们介绍开发的AV ChiME3语料库和feature提取流程。

### 3.1 Dataset

AV ChiME3语料库是通过将干净的Grid视频[17]与ChiME3噪声[28]（咖啡厅，街道交界处，公共交通（BUS），行人区域）混合而得到的，SNR范围为-12至12dB。预处理包括句子对齐和先前视觉框架的合并。执行句子对齐以从视频中删除静音时段，并防止模型学习多余或无关紧要的信息。先前的多个视觉帧用于合并时间信息以改善视觉和音频特征之间的映射。Grid语料库由34位演讲者组成，每位演讲者背诵1000句话。在34位发言者中，选择了5位发言者的子集（两名女性白人，两名男性白人和一名黑人男性），每位共有900条命令语句。该子集公平地确保了说话人独立性标准。表1给出了获取的视觉数据集的摘要，其中清楚定义了完整和对齐的句子，句子的总数，使用的句子和删除的句子。

Table 1: 使用的Grid语料库的句子

Table 2: 训练，测试与开发句子的总结

### 3.2. Audio feature extraction

使用广泛使用的log-FB向量提取音频特征。对于log-FB向量计算，输入音频信号以50kHz采样，并分成$N$个16ms帧，每帧800个采样，增量率为62.5％。此后，使用汉明窗和傅立叶变换来产生2048个单元的功率谱。最后，应用23维对数FB，然后对数压缩以产生23维对数FB信号。

### 3.3. Visual feature extraction

 使用基于2D-DCT的标准和广泛使用的视觉特征提取方法从以25 fps录制的Grid Corpus视频中提取视觉特征。首先，对视频文件进行处理以提取一系列单个帧。第二，Viola-Jones嘴唇检测器[29]用于通过定义边界框的兴趣区域（ROI）来识别嘴唇区域。使用基于Haar特征的级联分类器执行对象检测。该方法基于机器学习，其中使用正负图像训练级联函数。最终，对象跟踪器[30]用于跟踪帧序列中的嘴唇区域。视觉提取程序为每个帧生成一组边角点，然后通过裁剪原始图像来提取嘴唇区域。另外，为了确保良好的唇部追踪，通过检查每个句子中的几个帧来手动验证每个句子。手动验证的目的是删除那些未正确识别嘴唇区域的句子[31]。嘴唇跟踪优化超出了本文的范围。完整的自主性系统的开发以及对具有挑战性的数据集的测试正在进行中。

## 4. Experimental Results

### 4.1. Methodology

AV切换模块在范围广泛的从-9dB到9dB的带噪的AV ChiME3数据集上进行了训练。数据集的一个子集用于神经网络训练（70％），其余30％用于网络测试（20％）和验证（10％）。表2总结了Train，Test和Validation语句。对于泛化（独立于SNR）测试，该框架针对-9至6dB的SNR进行了训练，并针对9dB SNR进行了测试。

### 4.2. Training and Testing Results

在本小节中，将所提出的上下文AV模型的训练和测试性能与仅A和仅V模型进行了比较。初步的数据分析表明，V-only模型首先使用不同数量的过去视觉帧进行训练和测试，范围从1、2、4、8、14和18个视觉帧。MSE结果如表3所示。可以看出，通过从1个视觉帧移动到18个视觉帧，可以显着提高性能。具有1个视觉框架的LSTM模型的MSE为0.092，而具有18个视觉框架的LSTM模型的最小MSE为0.058。基于LSTM的学习模型有效地利用了时间信息（即先前的视觉框架），并显示了从1到18个视觉框架时MSE的一致减少。这主要是由于其固有的循环架构特性以及通过使用单元门可以长时间保持状态的能力。图4中显示了仅A（A2A），仅V（V2A）和AV（AV2A）的验证结果。请注意，所有三个模型均使用之前的14帧数据集（对于唇动时间相关性开发）。可以看出，AV上下文模型在实现最小MSE方面优于仅A和仅V模型。

Table3：针对不同过去视觉帧的LSTM（仅V型）训练和测试准确度比较

### 4.3. Speech Enhancement

#### 4.3.1. Objective Testing

对于客观测试，PESQ测试用于恢复语音质量评估。PESQ分数在-0.5至4.5范围内，代表最低和最佳的重建语音质量。图5描绘了针对不同SNR的EVWF（仅具有A，仅V和上下文AV映射），SS和LMMSE的PESQ得分。可以看出，在低SNR级别下，仅具有A的EVWF具有AV，仅V语言的效果明显优于基于SS [32]和LMMSE [33]的语音增强方法。在高SNR时，仅A和仅V的性能与SS和LMMSE方法相当。还应注意的是，在低SNR时，仅V优于仅A，在高SNR时，仅A优于仅V，从而验证了在高SNR时视觉效果较差而在低SNR时音频效果较差的现象。相比之下，在低和高SNR时，上下文AV EVWF明显优于基于SS和LMMSE的语音增强方法。ChiME3语料库的PESQ分数较低，尤其是在高SNR情况下，可归因于ChiMe3噪声的性质。后者的特征在于频谱时变，可能会降低增强算法恢复信号的能力。但是，AV上下文切换组件可以更好地处理频谱时变。图6展示了纯净的，SS，LMMSE，仅V，仅A和上下文AV增强音频信号的频谱图。可以看出，在低信噪比（-9db）下，仅有A，仅有V和AV的EVWF优于SS和LMMSE。显然，仅A，仅V和AV的频谱图没有太大区别。然而，仔细检查发现，仅V方法比仅A方法更好地近似了某些频率分量。3D彩色频谱图中突出显示了特定时频单位处的能量。

Figure5：带有ChiME3噪声的PESQ结果。在低信噪比和高信噪比级别上，带上下文AV模块的EVWF明显优于SS，LMMSE，仅A和仅V语音增强方法。在低SNR时，仅V的性能优于仅A；在高SNR时，仅A的性能优于仅V。

#### 4.3.2. Subjective Listening Tests

为了检验所提出的上下文AV语音增强算法的有效性，使用AV ChiME3语料对具有自我报告的正常听者的MOS进行了主观听觉测试。为听众提供单一刺激（即仅增强语音），并要求他们以1到5的等级对重新构造的语音进行评分。五个评分选择为：（5）优秀（当听众与目标相比感觉不到明显的差异时）言简意））（4）良好（可察觉但不烦人）（3）一般（轻微烦人）（2）较差（烦人）和（1）较差（非常烦人）。将带有上下文AV模块的EVWF与仅A，仅V，SS和LMMSE进行了比较。共有15位听众参加了此次评估会议。ChiME3噪声破坏了干净的语音信号（在SNR为-9dB，-6dB，-3dB，0dB，3dB，6dB和9dB的情况下）。图7描绘了具有ChiME3噪声的MOS方面的五种不同语音增强方法的性能。与PESQ结果相似，可以看出，在低SNR水平下，具有上下文AV，仅A和仅V模型的EVWF明显优于SS和LMMSE方法。在高信噪比，仅A和仅V的性能与SS和LMMSE方法相当。在低SNR时，仅V的性能优于仅A和在高SNR时，仅A的性能优于仅V的V，从而验证了在高SNR时视觉提示效果较差的现象，在低SNR时音频提示效果较差的现象。但是，即使在高SNR的情况下，具有上下文AV模型的EVWF仍远胜过SS，LMMSE，仅A和仅V的方法，可以在上下文中充分利用音频和视频提示的互补优势。

## 5 Conclusion

在本文中，提出了一种新颖的视音频切换组件，该组件可以在上下文中利用视觉和带噪的音频特征来近似估计不同噪声暴露下的干净音频特征。我们使用新颖的EVWF研究了我们提出的上下文AV开关的特征，并将其与仅A和仅V的EVWF进行了比较。EVWF重建干净语音信号的能力与AV映射精度（即干净音频特征估计）成正比，映射精度更好，可以为高质量语音重建设计更优化的维纳滤波器。实验结果表明，所提出的上下文视音频切换模块成功地利用了音频和视觉提示，以更好地估计各种带噪环境中的干净音频特征。在低和高SNR的语音质量和清晰度方面，具有上下文AV模型的EVWF优于基于A的EVWF，基于V的EVWF，SS和LMMSE的语音增强方法。在低SNR时，仅A和仅V的性能优于SS和LMMSE，但在高SNR时性能相当。此外，在低SNR时，仅V的性能优于仅A的性能；在高SNR时，仅V的性能优于仅V的性能。它验证了在高SNR时效果不佳的视觉提示和在低SNR时效果不佳的音频提示的现象。但是，具有上下文AV模型的EVWF可以更好地解决低和高SNRS时的频域-时域变化。有趣的是，在3D彩色频谱图结果中，可以观察到视觉提示比仅使用A更好地逼近了某些频率分量，其中在特定的时频下强度更大。将来，我们打算在不依赖说话者的情况下进一步研究我们提出的上下文AV模块和EVWF在现实环境中的性能。但是，对于证明概念，当前的研究相当满足说话人独立性标准，其中考虑了5个不同主题的子集（两个白人女性，两个白人男性和一个黑人男性），每个说话者总共有900个命令语句。
