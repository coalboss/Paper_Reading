# Looking to Listen at the Cocktail Party: A Speaker-Independent Audio-Visual Model for Speech Separation

这是一篇来自google的研究者们的关于说话者无关的音视频语音分离的创新型论文，发表时间2018年8月，被 SIGGRAPH 2018接收。

本文的贡献有二：

1. 发布了一个较大的音视频语料库，来源于Youtube，大约4700小时的视频片段，其中约有15万名不同的说话者。
2. 提出了一个基于Dilated Convolutional的Multi-stream结构，用于语音分离。根据给定的视频流，返回相应的音轨，并对此结构做了详细的消融分析。

## Abstract

Figure 1. 我们提出了一种用于隔离和增强视频中所需说话者语音的模型。（a）输入是一个视频(帧+音轨)，其中有一个或多个人在讲话，其中感兴趣的语音受到其他说话人和/或背景噪声的干扰。（b）提取音频和视觉特征并将其输入到联合视听语音分离模型中。输出是将输入音轨分解为干净的音轨，每个视频中检测到的人都有一个清晰的语音轨道（c）这样，我们就可以创作视频，在其中增强特定人的语音而抑制其他所有声音。我们使用新数据集AVSpeech中的数千小时视频片段对模型进行了训练。“Stand-Up”视频（a）由Team Coco提供。

我们提出了一种联合视听模型，用于将声音信号从其他说话者和背景噪声等混合声音中隔离出来，仅使用音频作为输入来解决此任务非常具有挑战性，并且无法提供分离的语音信号与说话者中的说话者的关联视频。在本文中，我们提出了一个基于深度网络的模型，该模型结合了视觉和听觉信号来解决此任务。视觉功能用于将音频“聚焦”到场景中所需的说话者上，并改善语音分离质量。为了训练我们的联合视听模型，我们引入了AVSpeech，这是一个新数据集，其中包含来自网络的数千小时的视频片段。我们展示了我们的方法适用于经典语音分离任务以及涉及激烈访谈，嘈杂酒吧和尖叫儿童的真实场景的适用性，仅要求用户在视频中指定他们想要隔离的人的脸部即可。在混合语音的情况下，我们的方法显示出优于最新的纯音频语音分离的优势。此外，我们的模型与说话者无关（训练过一次，适用于任何说话者），产生比最近说话者有关的视听语音分离方法更好的结果（需要为每个感兴趣的说话者训练一个单独的模型）。

## 1 Introduction

人类非常有能力将听觉注意力集中在嘈杂环境中的单个声音源上，同时不强调（“静音”）所有其他声音和声音。神经系统实现这一壮举的方式，即鸡尾酒会效应[Cherry 1953]，目前尚不清楚。然而，研究表明，在嘈杂的环境中观看说话者的脸可以增强人们解决感知歧义的能力[Golumbic等，2013； Ma等，2009]。在本文中，我们通过计算实现了此功能。

自动语音分离-将输入音频信号分离成其各个语音源-在音频处理文献中已有很好的研究。由于此问题天生是不适当的，因此需要先验知识或特殊的麦克风配置才能获得合理的解决方案[McDermott 2009]。此外，纯音频语音分离的一个基本问题是标签排列问题[Hershey et al.2016]：没有轻松的方法将每个分离的音频源与其视频中的相应说话者关联起来[Hershey et al.2016; Yu等。2017]。

在这项工作中，我们提出了一种联合视听方法，用于将音频“聚焦”在视频中所需的说话人上。然后可以对输入视频进行重组，以增强与特定人相对应的音频，同时抑制所有其他声音（图1）。更具体地说，我们设计和训练了一个基于神经网络的模型，该模型将记录的声音混合以及视频中每帧中检测到的人脸的紧密裁剪作为输入，并将混合后的内容分成每个检测到的说话者的单独音频流。该模型将视觉信息用作改善信号源分离质量（与纯音频结果相比）的一种手段，并将这些分离的语音轨道与视频中的可见说话人相关联。用户所需要做的就是指定他们想听视频中人的哪些面孔。

为了训练我们的模型，我们从YouTube收集了290,000个高质量的演讲，TED演讲和操作视频，然后从这些视频中自动提取大约4700个小时的视频片段，其中包含可见的说话人和清晰的语音，没有干扰的声音（图2）。我们称呼新数据集为AVSpeech。有了这个数据集，我们生成了“synthetic cocktail parties”的训练集-带有清晰语音的面部视频，其他语音音轨和背景噪音的混合物。

我们通过两种方式证明了我们的方法相对于最近的语音分离方法的好处。首先，我们展示了与纯语音混合物上最新的纯音频方法相比更好的结果。其次，我们展示了我们的模型在实际场景中能够从包含重叠语音和背景噪声的混合物中产生增强的声音流的能力。

总而言之，本文做出了两个主要贡献：（a）优于音频的视听语音分离模型仅用于经典语音分离任务的视听模型，适用于具有挑战性的自然场景。据我们所知，本文是第一个提出独立于说话者的视听模型进行语音分离的论文。（b）经过仔细收集和处理的新的大规模视听数据集AVSpeech，由视频片段组成，其中可听见的声音属于每个个人，在视频中可见，没有音频背景干扰。以获得语音分离方面的最新成果，并且可能对研究团体进行进一步的研究有用。我们的数据集，输入和输出视频以及其他补充材料都可以在项目网页<https://looking-to-listen.github.io/>上找到。

## 2 Related Work

我们简要回顾了语音分离和视听信号处理领域的相关工作。

语音分离。语音分离是音频处理中的基本问题之一，并且在过去的几十年中一直是广泛研究的主题。Wang和Chen [2017]全面概述了基于深度学习的新型纯音频方法，该方法可同时解决语音降噪问题[Erdogan，2015； Weninger，2015]和语音分离任务。

已经出现了两个最近的工作，它们解决了上述标签排列问题，从而在单声道情况下执行了独立于说话者的多说话者分离。Hershey等人[2016]提出了一种称为“深度聚类”的方法，该方法使用经过区别训练的语音embedding来聚类和分离不同的来源。Hershey等人[2016]也介绍了无置换或置换不变损失函数的想法，但他们发现它没有很好的工作。Isik等人[2016]和Yu等[2017]随后介绍了成功使用置换不变损失函数训练DNN的方法。

我们的方法相对于此类纯音频方法具有三方面的优势：首先，我们证明了视听模型的分离结果比最新的纯音频模型具有更高的质量。第二，我们的方法在混合有背景噪音的多个说话人的设置中表现良好，据我们所知，还没有令人满意的仅音频方法能够解决。第三，我们共同解决了两个语音处理问题：语音分离和将语音信号分配给其对应的面孔，到目前为止，这些问题已经分别解决了[Hooveret al. 2017; Hu et al. 2015; Monaci 2011]。

视觉和语音。使用神经网络对听觉和视觉信号进行多模态融合以解决各种与语音相关的问题的兴趣日益浓厚。这些包括视听语音识别[ Feng et al.2017; Mroueh et al.2015; Ngiamet al.2011]，通过无声视频（唇读）预测语音或文本[Chung et al.2016; Ephrat et al.2017]，以及从视觉和语音信号中进行无监督的语言学习[Harwath et al.2016]。这些方法利用了同时录制的视觉和听觉信号之间的自然同步。

视听（AV）方法也已用于语音分离和增强[Hershey et al.2004; Hershey and Casey2002; Khan 2016; Rivet et al.2014]。Casanovas等人[2010]使用稀疏表示来执行AV源分离，这是由于依赖于单独活动的区域来学习源特性，并且假定所有音频源都可以在屏幕上看到，因此受到限制。最近的方法已经使用神经网络来执行任务。Hou等[2018]提出了一种基于多任务CNN的模型，该模型输出去噪语音频谱图以及输入口区域的重构。Gabbay等人[2017]在视频上训练语音增强模型，其中目标说话者的其他语音样本用作背景噪声，采用的方案称为“noise-invariant training”。在最近工作中，Gabbay等人[2018]使用视频到声音的合成方法来过滤嘈杂的音频。这些AV语音分离方法的主要局限性在于它们依赖于说话者，这意味着必须为每个说话者分别训练专用模型。尽管这些作品做出了特定的设计选择，仅将其适用性限制在仅取决于说话者的情况下，但我们推测，至今仍未广泛研究与说话者无关的AV模型的主要原因是缺乏训练此类模型的足够大且多样化的数据集-像我们在此工作中构建并提供的数据集。据我们所知，本文是第一个解决与说话者无关的AV语音分离问题的论文。我们的模型能够分离和增强从未见过的说话者，他们使用的语言不是培训集的一部分。此外，我们的工作是独一无二的，因为我们可以在以前的纯音频和视听语音分离工作无法解决的环境中，在真实的示例中显示高质量的语音分离

图2. AVSpeech数据集：我们首先收集了290,000个高质量的在线在线公共视频，其中包括演讲和讲座（a）.从这些视频中，我们提取出语音清晰的片段（例如，没有混合音乐，听众声音或其他说话者），并且在帧中可见说话者（有关处理的详细信息，请参见第3节和图3）。这样就产生了4700个小时的视频剪辑，每个人讲话都没有背景干扰（b）.此数据涵盖了各种各样的人物，语言和面部姿势，并在（c）中显示了分布（使用自动分类器估算的年龄和头角；基于YouTube元数据的语言）。有关我们数据集中视频源的详细列表，请参阅项目网页。

最近出现了许多独立的并发工作，这些工作解决了使用深度神经网络进行视听声源分离的问题。[Owens and Efros 2018]训练网络以预测音频和视频流是否在临时上对齐。然后，将从此自我监督模型中提取的学习特征用于调节开/关屏幕说话人源分离模型。Afouras等人[2018]通过使用网络预测降噪后的语音频谱图的幅度和相位来执行语音增强。Zhao[2018] and Gao [2018]解决了分离多个屏幕对象（例如乐器）的声音的紧密相关的问题。

视听数据集。大多数现有的AV数据集都包含仅包含少量主题的视频，这些视频来自有限词汇表。例如，CUAVE数据集[Patterson et al.2002]包含36个主题，每个主题从0到9次表示每个数字，每个数字总共180个示例。另一个例子是Hou等人[2018]引入的普通话句子数据集，其中包含由母语使用者说​​的320普通话句子的视频记录。每个句子包含10个具有相同音素分布的汉字。TCD-TIMIT数据集[Harte and Gillen 2015]由60位自愿讲者组成，每人约200个视频。演讲者从TIMIT数据集中朗读各种句子[S Garofolo et al.1992]，并使用前置摄像头和30度摄像头录制。我们将对这三个数据集的结果进行评估，以便与以前的工作进行比较。

最近，Chung等人[2016]引入了大规模的唇读句（LRS）数据集，其中既包括各种各样的说话者，也包括来自更大词汇量的单词。然而，不仅数据集不是公开可用的，而且不能保证LRSvideos中的语音清晰，这对于训练语音分离和增强模型至关重要。

## 3 AVSPEECH Dataset

我们引入了一个新的大规模视听数据集，该数据集包括没有干扰背景信号的语音剪辑。这些片段的长度是不同的，介于3到10秒之间，并且在每个剪辑中，视频中唯一可见的面孔和音轨中的可听声音都属于单个讲话者。总体而言，数据集包含大约4700小时的视频片段，其中约有15万名不同的演讲者，涵盖了各种各样的人物，语言和面部姿势。图2中显示了代表性的帧，音频波形和某些数据集统计数据。

我们自动收集了数据集，因为要组装如此大小的语料库，重要的是不要依赖大量的人类反馈。我们的数据集创建渠道从大约290,000个YouTube演讲，视频（如TED演讲）视频和操作视频中收集了片段。对于这样的频道，大多数视频都包含一个说话者，并且视频和音频通常都是高质量的。

Figure3. 用于创建数据集的视频和音频处理：（a）我们使用面部检测和跟踪从视频中提取语音片段候选，并拒绝面部模糊或没有足够正面朝向的帧。（b）丢弃语音嘈杂的片段通过估计语音SNR（请参阅第3节）。该图旨在显示我们的语音SNR估算器的准确性（以及数据集的质量）。对于已知语音SNR级别下的干净语音和非语音噪声的合成混合物，我们将真实语音SNR与预测的SNR进行比较。预测的SNR值（以dB为单位）是每个SNR仓中60个生成的混合信号的平均值，误差条表示1 std。我们丢弃了预测的语音SNR低于17 dB（图中灰色虚线所示）的段。

数据集创建流程。我们的数据集收集过程分为两个主要阶段，如图3所示。首先，我们使用了Hoover等人的演讲者跟踪方法[2017]。检测一个人的视频片段，使他们的面部可见并积极讲话。从片段中丢弃了模糊，照明不足或姿势极端的脸部框架。如果某个片段中超过15％的脸框丢失，则将其全部丢弃。在此阶段，我们将Google Cloud Vision API用作分类器，并计算出图2中的统计数据。

建立数据集的第二步是完善语音段，使其仅包含干净，无干扰的语音。这是非常重要的组成部分，因为这样的部分在训练过程中充当了基础真理。我们通过如下估算每个段的语音SNR（主要语音信号与其余音频信号的对数比）自动执行此优化步骤。

我们使用经过预训练的纯音频语音降噪网络，使用降噪后的输出作为纯信号的估计来预测给定段的SNR。该网络的体系结构与第5节中针对纯音频增强语音的基线实现的体系结构相同，并且接受了来自公共领域有声读物的LibriVox集合中的语音的培训。拒绝估计信噪比低于阈值的线段。该阈值是使用干净语音和非语音干扰噪声在不同已知SNR下的合成混合物凭经验设置的。这些合成的混合物被送入去噪网络，并将估计的（去噪的）SNR与真实的SNR进行比较（见图3（b））。

我们发现，在低SNR时，平均而言，估计的SNR非常准确，因此可以认为是原始噪声水平的良好预测指标。在较高的SNR（即原始语音信号几乎没有干扰的段）下，此估计器的准确性会降低，因为噪声信号很微弱。如图3（b）所示，发生这种情况的阈值约为17 dB。我们听取了100个经过此过滤的剪辑的随机样本，发现其中没有一个明显的背景噪声。我们在补充资料中提供了来自数据集的示例视频剪辑。

## 4 AUDIO-VISUAL SPEECH SEPARATION MODEL

从高层次上讲，我们的模型由多流体系结构组成，该体系结构将检测到的人脸的视觉流和嘈杂的音频作为输入，并输出复杂的频谱图mask，视频中每个检测到的人一个mask（图4）。然后将带噪声的输入频谱图乘以mask，以得到每个说话人的隔离语音信号，同时抑制所有其他干扰信号。

### 4.1 Video and Audio Representation

输入特征。我们的模型将视觉和听觉特征都作为输入。给定一个包含多个说话人的视频片段，我们使用现成的面部检测器（例如Google Cloud Vision API）在每个帧中查找面部（每个说话人总共75张面部缩略图，假设25 fps时3秒的片段）。我们使用预训练的人脸识别模型为每个检测到的人脸缩略图每帧提取一个人脸embedding。我们使用网络中没有空间变化的最低层，类似于Cole等人[2016]使用的那一层。用于合成脸部。这样做的理由是，这些embedding保留了识别数百万张脸部所需的信息，同时丢弃了图像之间不相关的变化（例如照明）。实际上，最近的工作还表明，有可能从这种embedding中恢复面部表情[Ruddet al.2016]。我们还对人脸图像的原始像素进行了实验，但这并没有提高性能。

至于音频特征，我们计算了3秒音频片段的短时傅立叶变换（STFT）。每个时频（TF）单元包含复数的实部和虚部，我们将两者均用作输入。我们执行power-law压缩，以防止响亮的音频压倒柔和的音频。带噪信号和纯净参考信号均采用相同的处理。

在推论时，我们的分离模型可以应用于任意长的视频片段。当在一帧中检测到多个说话的面部时，我们的模型可以接受多个面部流作为输入，我们将在稍后讨论。

输出：我们模型的输出是一个乘法频谱图mask，它描述了净语音与背景干扰的时频关系。在以前的工作中[Wang and Chen 2017； Wang et al.2014]，观察到乘法mask比替代方法更好，例如直接预测频谱图幅值或直接预测时域波形。源分离文献[Wang and Chen 2017]中存在基于mask的训练目标的许多类型，其中我们尝试了两种：比率mask（RM）和复杂比率mask（cRM）。

理想比率mask定义为干净声谱图和嘈杂声谱图的幅值之间的比率，并假定介于0和1之间。复数理想比率mask定义为复数干净和嘈杂的频谱图的比率。cRM具有实部和虚部，它们在实域中分别估算。复数mask的实部和虚部通常在-1和1之间，但是，使用sigmoid将这些复数mask值限制在0和1之间。

使用cRM进行mask时，通过对预测的cRM和噪声频谱图进行复数乘法，可以通过执行逆STFT（ISTFT）获得降噪波形。使用RM时，在预测的RM和噪声频谱图幅度的点乘上使用ISTFT，并结合噪声原始相位[Wang and Chen 2017]。

考虑到多个检测到的说话者的面部流作为输入，网络会为每个说话者输出一个单独的mask，以及一个用于背景干扰。我们发现大多数使用cRM的实验，因为我们发现使用cRM的输出语音质量明显优于RM。两种方法的定量比较请参见表6。

Figure 4：我们模型基于多流神经网络的架构：视觉流将视频中每个帧中检测到的面部作为缩略图输入，而音频流将视频的音轨作为输入，其中包含语音和背景噪声的混合。视觉流使用预先训练的面部识别模型为每个缩略图提取面部embedding，然后使用膨胀卷积神经网络学习视觉特征。音频流首先计算输入信号的STFT以获取频谱图，然后使用类似的膨胀卷积神经网络学习音频表示。然后，通过级联学习到的视觉和音频特征来创建联合的视听表示，然后使用双向LSTM和三个完全连接的层对其进行进一步处理。网络为每个说话人输出一个复数频谱图mask，该mask与噪声输入相乘，然后转换回波形以获得每个说话人的隔离语音信号。

Table1. 构成模型音频流的膨胀卷积层

Table2. 构成模型视觉流的膨胀卷积层

### 4.2 Network architecture

图4提供了我们网络中各个模块的高级概述，下面我们将对其进行详细描述。

音频和视频流。我们模型的音频流部分由膨胀的卷积层组成，其参数在表1中指定。

我们模型的视觉流用于处理输入的面部embedding（请参阅第4.1节），由表2中详细说明的膨胀卷积组成。请注意，视觉流中的“空间”卷积和膨胀是在时间轴上执行的（而不是在1024维的人脸embedding通道上执行）

为了补偿音频和视频信号之间的采样率差异，我们对视觉流的输出进行上采样以匹配频谱图采样率（100Hz）。这是通过在每个视觉特征的时间维度上使用简单的最近邻插值完成的。

AV融合。通过串联每个流的特征图来合并音频和视频流，然后将其映射到BLSTM和三个FC层中。最终输出包括每个输入说话人的复数mask（实数和虚数两个通道）。相应的频谱图是通过对有噪声的输入频谱图和输出mask进行复数乘法来计算的。幂律压缩后的干净频谱图和增强频谱图之间的平方误差（L2）被用作损失函数来训练网络。如4.1节所述，使用ISTFT获得最终输出波形。

多个说话人。我们的模型支持隔离视频中的多个可见说话人，每个说话人由一个视觉流表示，如图4所示。一个单独的专用模型针对每种可见说话人（例如，一个具有一个视觉流的模型（用于一个可视说话人），一个双视觉流模型的（用于两个说话人）等。所有视觉流在卷积层上共享相同的权重。在这种情况下，在继续进入BLSTM之前，将从每个视觉流中学习到的特征与学习到的音频特征相连接。应当注意的是，实际上，可以将以单个视觉流作为输入的模型用于一般情况，即说话者数量未知或专用的多说话者模型不可用时。

### 4.3 Implementation details

我们的网络是在TensorFlow中实现的，其包含的操作用于执行波形和STFT转换。ReLU激活遵循除最后一个（mask）之外的所有网络层，最后一层使用了Sigmoid。在所有卷积层之后执行批归一化[Ioffe and Szegedy2015]。由于我们训练大量数据并且不会遭受过度拟合的困扰，因此未使用Dropout。我们使用6个样本的批次大小，并使用Adam优化器进行500万步（batch）的训练，学习速率为3e-5，每180万步减少一半。

将所有音频重新采样到16kHz，并且仅通过左声道将立体声音频转换为单声道。STFT使用长度为25ms，跳长为10ms，FFT大小为512的Hann窗口计算得出，因此输入音频特征为$257\times298\times2$标量。幂律压缩在$p = 0.3$（$A^{0.3}$，其中A是输入/输出音频频谱图）。

在训练和推理之前，我们通过移除或复制embedding来将所有视频的面部embedding重新采样为25帧/秒（FPS）。这将产生75个面部embedding的输入视觉流。面部检测，对准和质量评估是使用Cole等人[2016]描述的工具进行的。当在特定样本中遇到丢失帧时，我们使用零向量代替面部embedding。

## 5 EXPERIMENTS AND RESULTS

我们在各种条件下测试了我们的方法，并在定量和定性上将我们的结果与最新的纯音频（AO）和视听（AV）语音分离和增强进行了比较。

与仅音频的比较：没有公开可用的最先进的纯音频语音增强/分离系统，很少有公开可用的训练和评估纯音频语音增强的数据集。而且，尽管有大量文献报道“盲源分离”用于仅音频语音的增强和分离[Comon and Jutten 2010]，但是这些技术大多数都需要多个音频通道（多个微型麦克风），因此不适用于我们的任务。对于这些原因，我们实现了用于语音增强的AO基准，该基准与我们的视听模型中的音频流具有相似的体系结构（当去除可视流时，图4）。在广泛用于语音增强工作的CHiME-2数据集[Vincent等，2013]上进行训练和评估时，我们的AO基线达到了14.6 dB的信噪比，几乎达到了Erdogan等人[2015]报告的最新单通道结果为14.75 dB。因此，我们的AO增强模型被认为是接近最新水平的基准。

为了将我们的分离结果与最先进的AO模型的分离结果进行比较，我们实施了Yu等人[2017]引入的置换不变训练。请注意，使用此方法进行语音分离需要事先知道录制中存在的来源数量，并且还需要将每个输出通道手动分配给视频中相应说话人的脸部（我们的AV方法会自动执行此操作）。

我们将在第5.1节的所有综合实验中使用这些AO方法，并在第5.2节中对真实视频进行定性比较。

与最新的视听方法的比较。由于现有的语音分离和增强方法是取决于说话者的，因此我们无法轻松地在合成混合物的实验中（第5.1节）将其与它们进行比较，也无法在自然视频中运行它们（第5.2节）。但是，通过在这些论文的视频上运行我们的模型，我们可以在现有数据集上显示与这些方法的定量比较。我们将在5.3节中详细讨论这种比较。此外，我们在补充材料中进行了定性比较。

### 5.1 Quantitative Analysis on Synthetic Mixtures

我们为几种不同的单通道语音分离任务生成了数据。每个任务都需要自己独特的语音和非语音背景噪声混合配置。我们在下面描述每种训练数据变体的生成过程，以及从头开始训练的每种任务的相关模型。

在所有情况下，干净的语音剪辑和相应的面孔都来自我们的AVSpeech（AVS）数据集。从AudioSet [Gemmeke et al.2017]获得了非语音背景噪声，这是来自YouTube视频的手动注释片段的大规模数据集。分离语音质量是使用BSS Eval工具箱中的信号失真比（SDR）实现进行评估的 [Vincent et al.2006]，通常用于评估语音分离质量的度量标准（请参阅附录A部分）。

我们从数据集中的变长片段中提取了3秒的非重叠片段（例如，一个10秒的片段将贡献3个3秒的片段）。我们为所有模型和实验生成了150个种合成混合物。对于每个实验，将生成的数据的90％用作训练集，其余的10％用作测试集。我们未使用任何验证集，因为未执行任何参数调整或提早停止操作。

一个说话人噪声（1S噪声）。这是一项经典的语音增强任务，其训练数据是通过非标准化干净语音和AudioSet噪声的线性组合生成的：$Mix_i=AVS_j+0.3*AudioSet_k$其中$AVS_j$是来自AVS的一条句子，$AudioSet_k$是音频集的的片段,幅度乘以0.3，$Mix_i$是生成的合成混合物数据集中的样本。在这种情况下，我们的纯音频模型表现很好，因为噪声的特征频率通常与语音的特征频率很好地分开。我们的视听（AV）模型的性能与纯音频（AO）基线相同，SDR为16 dB（表3的第一列）。

两个干净的说话人（2S clean）。此两个说话人分离场景的数据集是通过将来自我们的AVS数据集中的两个不同说话人的干净语音混合而成的：$Mix_i=AVS_j+AVS_k$，其中，$AVS_j$和$AVS_k$是来自我们数据集中不同来源视频的干净语音样本， $Mix_i$是生成的合成混合物数据集中的一个样本。除了AO基准外，我们还为此任务训练了两个不同的AV模型:
（i）仅采用一个视觉流作为输入，并且仅输出其对应的去噪信号的模型。在这种情况下，可以推断出每个说话人的降噪信号是通过网络中的两次前向通行获得的（每个说话人一个）。平均此模型的SDR结果比我们的AO基线（表3的第二列）改善了1.3 dB。
（ii）一个模型，它以两个独立的流（如第4节中所述）的方式将来自两个说话人的视觉信息作为输入。在这种情况下，输出由两个mask组成，每个说话人一个，而推理是通过一次前向通过来完成的。使用此模型可获得0.4 dB的额外增强，从而使总SDR改善10.3 dB。直观上，联合处理两个可视流将为网络提供更多信息，并对分离任务施加更多约束，从而改善结果。

图5显示了针对纯音频基准线和我们的两个说话人视听模型，针对此任务的SDR随输入SDR的变化情况。

Table3：仅音频语音分离和增强的定量分析和比较：质量改进（在SDR中，请参见附录A节）是使用不同网络配置的输入视觉流数量的函数。第一行（仅音频）是我们对最先进的语音分离模型的实现，并显示为基线

Figure5：输入SDR与输出SDR的改善：散点图显示了分离性能（SDR的改善）与原始（嘈杂）SDR的关系，用于分离两个干净的说话人（2S干净）。每个点对应于测试集中一个3秒的视听样本。

Figure6：输入和输出音频的示例：第一行显示了我们训练数据中一个片段的音频频谱图，涉及两个说话人和背景噪声（a），以及真实情况，每个说话人的独立频谱图（b， C）。在最下面的行中，我们显示了结果：掩盖了我们对该段的方法估计值，将它们叠加在一个频谱图上，每个说话者（d）具有不同的颜色，而每个说话者的相应输出频谱图（e，f）

两个说话者+噪声（2S噪声）。在此，我们考虑将一个说话者的声音与两个说话者和非语音背景噪声的混合物隔离开来的任务。就我们所知，此视听任务以前从未解决过。训练数据是通过将两个不同说话者的清晰语音（为2S清理任务生成的）与来自AudioSet的背景噪声混合而生成的：$Mix_i=AVS_j+AVS_k+0.3*AudioSet_l$。

在这种情况下，我们训练有三个输出的AO网络，每个说话人一个，一个用于背景噪声。此外，我们训练了模型的两种不同配置，选择接收了一个和两个视觉流作为输入。单流AVmodel的配置与先前实验中的模型（i）相同。双流AV输出三个信号，每个说话人一个，一个是背景噪声。如表3（第三列）所示，我们的单流AV模型在仅音频基线上的SDR增益为0.1 dB，对于两个流为0.5 dB，使总SDR改进为10.6 dB。图6显示了此任务的样本段的推断mask和输出频谱图，以及其嘈杂的输入和真实频谱图。

三个干净的说话者（3S干净）。通过混合来自三个不同说话者的干净语音创建此任务的数据集：$Mix_i=AVS_j+AVS_k+AVS_l$。与之前的任务类似，我们使用一个，两个和三个视觉流作为输入来训练我们的AV模型，分别输出一个，两个和三个信号。

我们发现，即使使用单个视觉流，AV模型的性能也比AO模型好，并且比AO模型有0.5 dB的改进。两种视觉流配置与AO模型相比具有相同的改进，而使用三个视觉流带来1.4 dB的增益，总共实现了10 dB的SDR改善（表3的第四列）。

同性别分离。许多以前的语音分离方法在尝试分离包含同性别语音的混合语音时表现出下降[Delfarah and Wang 2017; Hershey et al.2016]。表4显示了按性别划分的分离质量细分。有趣的是，我们的模型在女女混合物上的表现最佳（略有提高），但在其他组合上的表现也很好，证明了其性别稳健性。

### 5.2 Real-World Speech Separation

为了演示我们模型在现实世界中的语音分离能力，我们在各种视频中对其进行了测试，其中包含激烈的辩论和访谈，嘈杂的酒吧和尖叫的孩子（图7）。在每种情况下，我们都使用经过训练的模型，其视觉输入流的数量与视频中可见说话者的数量相匹配。例如，对于具有两个可见说话人的视频，使用了两个说话人模型。由于我们的网络架构从不强制规定特定的时间长度，因此我们使用模型支持的每个视频单向前通过来执行分离，这使我们避免了对短视频块进行后处理和合并结果的需要。由于这些示例没有干净的参考音频，因此将对这些结果及其与其他方法的比较进行定性评估。它们在我们的补充材料中介绍。应该注意的是，我们的方法不能实时运行，以目前的形式，我们的语音增强功能更适合于视频编辑的后处理阶段。

我们的补充材料中的合成“Double Brady”视频突出了我们模型对视觉信息的利用，因为在这种情况下仅使用音频中包含的特征性语音频率很难进行语音分离。

“Noisy Bar”场景显示了我们从低SNR的混合物中分离语音的方法的局限性。在这种情况下，背景噪声几乎被完全抑制，但是输出语音质量明显下降。Sun等人[2017]观察到此限制源于使用基于mask的分离方法，并且在这种情况下，直接预测去噪频谱图可以帮助克服此问题。在经典语音增强的情况下，即一位具有非语音背景噪声的说话人，我们的AV模型获得的结果与强大的AO基准相似。我们怀疑这是因为噪声的特征频率通常与语音的特征频率很好地分开，因此合并视觉信息并不能提供额外的辨别能力。

### 5.3 Comparison with Previous Work in Audio-VisualSpeech Separation and Enhancement

如果不将我们的结果与先前在AV语音分离和增强方面的工作进行比较，我们的评估将是不完整的。表5包含在第2节中提到的三个不同的AV数据集（普通话，TCD-TIMIT和CUAVE）上的比较，使用相应论文中描述的评估协议和度量标准。报告的客观质量得分为PESQ [Rix等，2001]，STOI [Taal等，2010]和BSS evaltoolbox的SDR [Vincent等，2006]。这些比较的定性结果可在我们的项目页面上找到。

重要的是要注意，这些现有方法需要针对其数据集中的每个说话者训练特定模型（取决于说话者），而我们对他们的数据的评估是使用在我们的一般AVS数据集（独立于说话者）上训练的模型完成的。尽管以前从未遇到过这些特定的发言人，但我们的结果比原始论文中的结果要好得多，这表明我们的模型具有强大的泛化能力。

Table4：同性别分离。该表的结果来自2S清洁实验，表明我们的方法对于从同性别混合物中分离语音是鲁棒的。

Table5：与现有视听语音分离工作的比较。我们使用原始论文中报告的评估协议和客观评分，将我们在多个数据集上的语音分离和增强结果与以前的工作进行了比较。请注意，先前的方法是与说话者相关的，而我们的结果是通过使用通用的，与说话者无关的模型获得的。

### 5.4 Application to Video Transcription

虽然我们在本文中的重点是语音分离和增强，但我们的方法也可用于自动语音识别（ASR）和视频转录。作为概念证明，我们进行了以下定性实验。我们将针对“Stand-Up”视频的语音分离结果上传到了YouTube，并将YouTube自动字幕所产生的字幕与针对混合语音的相应源视频所产生的字幕进行了比较。对于原始“Stand-Up”视频的一部分，ASR系统无法在视频的混合语音段中生成任何字幕。结果包括两位演讲者的讲话，导致句子难以阅读。但是，在我们分开的语音结果上产生的字幕明显更准确。我们会在补充材料中显示完整的字幕视频。

### 5.5 Additional Analysis

我们还进行了广泛的实验，以更好地了解模型的行为以及模型的不同组成部分如何影响结果。

消融研究。为了更好地理解模型不同部分的作用，我们对从两个干净的说话人（2S Clean）的混合物中进行语音分离的任务进行了消融研究。除了消除网络模块的几种组合（视觉和音频流，BLSTM和FC层）之外，我们还研究了更高级的更改，例如不同的输出mask（幅值），将学习到的视觉特征减少到每个时间步一个标量的效果，以及不同的融合方法（早期融合）。

在早期的融合模型中，我们没有单独的视觉和音频流，而是将输入中的两种模态组合在一起。这是通过首先使用两个完全连接的层来减少每个视觉embedding的维数以匹配每个时间步的声谱图维度来完成的， 然后将视觉特征堆叠为第三频谱图“通道”，并在整个模型中对其进行共同处理。

表6显示了我们的消融研究结果。该表包括使用SDR和ViSQOL进行的评估[Hines et al.2015]，这是一种旨在逼近人类听众平均语音质量（MOS）的客观指标。ViSQOL分数是根据我们测试数据的随机2000个样本子集计算得出的。我们发现，SDR与分离的音频中残留的噪声量具有很好的相关性，ViSQOL是输出语音质量的更好指标。有关这些分数的更多详细信息，请参见附录中的A节。“Oracle” RM和cRM是如第4.1节中所述，通过分别使用实数实值和复值频谱图获得的mask。

这项研究最有趣的发现是使用实值幅度掩码而不是复杂的幅度掩码时MOS的下降，以及每个时间步长将视觉信息压缩为一个标量的惊人效果，如下所述。

瓶颈特征。在我们的消融分析中，我们发现，将视觉信息压缩到每个时间步长一个标量的瓶颈（“Bottleneck（cRM”）的网络的性能几乎与每个时间步使用64个标量的完整模型（“ Full模型（cRM）”）一样（0.5dB的减少）。

Figure7：野外语音分离：自然视频中的代表性帧在各种现实世界场景中展示了我们的方法。所有视频和结果都可以在补充材料中找到。“UndisputedInterview”视频由福克斯体育提供。

Table6：消融研究：我们研究了模型的不同部分对分离两个干净的说话人的场景的影响.SDR与噪声抑制有很好的相关性，ViSQOL表示语音质量水平（请参阅附录A节）

模型如何利用视觉信号？我们的模型使用面部embedding作为输入视觉表示（第4.1节）。我们希望深入了解这些高级特征中捕获的信息，并确定模型用于分离语音的输入帧的哪些区域。为此，我们遵循与[Zeiler and Fergus 2014; Zhou et al.2014]用于可视化深层网络的接收场。我们将该协议从2D图像扩展到3D（时空）视频。更具体地说，我们以滑动窗口的方式使用时空补丁遮挡器$(11px\times11px\times200ms\ patch)$。对于每个时空遮挡物，我们将遮挡的视频前馈到我们的模型中，并将语音分离结果$S_{occ}$与在原始（非遮挡）视频中获得的语​​音分离结果$S_{orig}$进行比较。

为了量化网络输出之间的差异，我们使用SNR，将没有遮挡物的结果视为“信号”。也就是说，对于每个时空补丁，我们计算：
$$
E=10 \cdot \log (\frac{S_{\text {orig}}^{2}}{(S_{\text {occ}}-S_{\text {orig}})^{2}}) \tag{1}
$$
对视频中的所有时空补丁重复此过程会导致每一帧的热图。出于可视化目的，我们通过视频的最大SNR对热图进行归一化：$\tilde{E}=E_{\max}-E$。在$\tilde{E}$中，高值对应于对语音分离结果有很大影响的patch。

在图8中，我们显示了来自几个视频的代表帧生成的热图（完整的热图视频可在我们的项目页面上找到）。正如预期的那样，贡献最大的面部区域位于嘴周围，但可视化显示其他区域（如眼睛和脸颊）也有贡献。

视觉信息缺失的影响。通过逐步消除视觉embedding，我们进一步测试了视觉信息对模型的贡献。具体来说，我们首先运行模型，然后使用视觉信息评估完整的3秒视频的语音分离质量。然后，我们逐渐从段的两端丢弃embedding，并以2、1、0.5和0.2秒的可视持续时间重新评估分离质量。

结果显示在图9中。有趣的是，当将语音嵌入段中的可视嵌入减少2/3时，语音分离质量平均仅降低0.8dB。这显示了模型对于丢失视觉信息的鲁棒性，在现实世界中，由于头部运动或遮挡，这些信息可能会丢失。

Figure8：模型如何利用视觉信号？我们显示了覆盖在多个视频的代表性输入帧上的热图，从而可视化了帧中不同区域对语音分离结果的贡献（以dB为单位，参见文本），从蓝色（低）贡献）变为红色（高贡献）

FIgure9：丢失视觉信息的影响：该图显示了视觉信息持续时间对2S clean场景中输出SDR改善的影响。我们通过将样本两端的输入面部嵌入逐渐归零来对此进行测试。结果表明，即使是少量的可视帧也足以实现高质量的分离。

## 6 CONCLUSION

我们提出了一个基于视听神经网络的模型，用于单通道，独立于说话者的语音分离。我们的模型在具有挑战性的场景中运作良好，包括具有背景噪声的多说话人混合。为了训练模型，我们创建了一个新的视听数据集，其中包含数千小时的视频片段，其中包含从网络上收集的可见说话者和清晰的语音。我们展示了有关语音分离的最新结果以及在视频字幕和语音识别中的潜在应用。我们还进行了广泛的实验，以分析模型及其组件的行为。
