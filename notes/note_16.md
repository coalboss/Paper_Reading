# Audiovisual Speech Source Separation: An overview of key methodologies

## Abstract

在嘈杂和混响环境中,仅使用音频模态分离在多个麦克风处测量的语音信号存在局限性，因为通常没有足够的信息来完全区分不同的声源。人类通过利用对背景噪声不敏感并且可以提供有关音频场景的上下文信息的视觉模态缓解该问题。这一优势启发了视听（AV）语音源分离新领域的创建，该领域的目标是在机器中使用麦克风同时测量视觉形态。在这个新兴领域的成功将使基于语音的机器接口（例如iPhone和iPad上的智能个人助手Siri）的应用范围扩大到更多实际设置，从而提供更自然的人机交互。

## Introduction

本文的目的是概述从早期使用音频模式识别语音活动的早期方法到综合完整AV模型的复杂技术，概述AV语音源分离构建中的关键方法。在信号处理的这一激动人心的领域确定得到了新方向。

仅观察到混合物的分离语音信号需要诸如盲源分离（BSS）之类的技术。在过去的二十年中，该主题已在信号处理社区中进行了广泛的研究，并已影响到许多应用，例如语音增强和机器听觉[1]。证明BSS应用的一个众所周知的例子是Cherry提出的所谓的鸡尾酒会问题[2]。他的愿望是制造一种机器，模仿人类将目标语音源与多种声音信号（包括干扰声音和背景噪声）的叠加分离的能力，这些声音信号通常与房间表面的声音反射相结合。考虑到分离模型中的房间混响，通常会在卷积BSS框架内解决此问题。在此框架中，矢量观测值$x(t)$被建模为矢量源$s(t)$的线性卷积混合物：$x(t)=H(t)*s(t)$，其中$H(t)$是每个源与每个混合之间的冲激响应矩阵以及$t$是离散时间索引。为简单起见，$H(t)$假定为方阵，即麦克风和信号源的数量相等，但这不是实现分离所必需的。因此，目的是估计解混合矩阵$W(t)$，以使得$\hat{s}(t)=W(t)x(t)$包含每个源$s_i(t)$的估计，其中下标$i$是源的索引。可替代地，通过将​​全频带语音混合物转换成子频带分量，然后将其单独地或联合地分离，在变换域中解决该问题，从而导致计算上更有效的方法，例如频域BSS。在后一种情况下，假设为静态源，则混合和解混合方程的频域对应项分别为$x(m,f)=H(f)s(m,f)$和$\hat{s}(m,f)=W(f)x(m,f)$，其中，$\cdot(m,f)$是$\cdot(t)$的短期离散时间傅立叶变换（STFT），$\cdot(f)$是$\cdot(t)$的傅立叶变换，$m$和$f$分别是时间帧索引和频点索引。但是，这就引入了排列和缩放不确定性问题，由于瞬时BSS模块固有的各个频段上分离的源分量的潜在顺序和比例可能不一致：换句话说，$W(f)=\Lambda(f) P(f) H(f)^{-1}$，其中$\Lambda(f)$是对角矩阵（即，对缩放不确定性建模），$P(f)$是置换矩阵（即，对置换歧义建模）。在重建全频带源信号之前，已经开发出许多方法来减轻这些歧义（更多细节可以在[1]中找到）。一种较新的方法是独立矢量分析（IVA），其中，通过跨所有频段进行自适应耦合，可以减轻置换问题的困扰[3]。

这些卷积BSS技术可以广泛归因于基于线性滤波的方法这一类。分离卷积混合物的另一种有效方法是基于使用时频（TF）mask的时变滤波形式，其目的是对于每个源形成概率（软）或二进制（硬）mask $\mathcal{M}(m,f)$，然后将mask应用于混合物的TF表示以提取该来源：$\hat{s}(m,f)=\mathcal{M}(m,f)*x(m,f)$。可以通过评估各种来自混合物的提示来估计掩码，诸如统计，空间，时间和/或频谱提示，在最大似然或贝叶斯框架下使用期望最大化（EM）算法[4]。T-F mask技术通常可以直接应用于欠定混合物中，以提取比观察到的信号更多的信号源。尽管在这些领域做出了这些努力并取得了令人鼓舞的进展，但是在以下两种实际情况下，最先进的算法通常会受到影响：高混响和带噪的环境，以及存在多个移动源的情况。例如，大多数现有的频域BSS方法实际上受到数据长度限制的约束，即，每个频点可用的样本数量不足以使学习算法收敛[5]，而各种线索，例如，用于计算TF mask估计中存在信号源的可能性的空间线索，随着混响和背景噪声的增加而变得更加模糊。在这些不利的声学环境中，大多数现有算法的性能会大大降低。

前面提到的方法仅利用音频域中的单个模态信号。然而，现在已经被广泛接受的是，人类语音固有地至少是双模态的，涉及音频和视觉模态之间的相互作用[6]。例如，发声活动通常与声学器官的视觉运动相结合，而阅读嘴唇的运动可以帮助人们在嘈杂的环境中推断口头句子的含义[7]。众所周知的McGurk效应还证实，视觉发音信息会自动，不自觉地集成到人类语音感知过程中[8]。例如，在某些条件下，通常将视觉/ga/与听觉/ba/结合起来称为/da/。正如Cherry [2]所建议的，融合来自不同感官测量的AV信息将是解决机器鸡尾酒会问题的最佳方法。先前已经利用了固有的AV一致性来改善自动语音识别[13]和识别[14]的性能。一致性一词在此用于描述音频和视觉模态之间的依赖性，与该术语在文献[9] – [12]等先前著作中的常规用法相一致。如后续部分所述，依赖关系可以建模为AV特征的联合分布或联合的AV原子（即信号分量）。

在[9]中的研究中，被白噪声破坏的语音信号通过视频输入估计的滤波器得到增强。目的是基于使用干净数据库训练的回归器的音频和视频信号之间的线性回归[线性预测编码（LPC）]来估计随时间变化的维纳滤波器，因此被称为AV-维纳过滤器。这项初步研究已证明对非常简单的数据（元音和辅音的成功）非常有效。例如，在输入信噪比（SNR）为-18 dB的情况下，对AV信号进行增强后，对滤波后的信号进行简单的线性判别分析可得出40％的词分类精度（CA）。具有经典音频增强功能的10％的CA，而未经滤波的数据导致5％的CA。但是，由于音频和视频信号之间的复杂关系，发现这种简单的方法在应用于更复杂的信号（例如自然语音和其他噪声源）时会受到限制。尽管如此，这种开创性的方法已经表明，在处理语音增强时，结合视频信息可能会非常有益，同时反映自动语音识别系统中获得的优势[13]。在过去的十年中，将视觉信息集成到纯音频语音源分离系统中已成为信号处理中令人兴奋的新领域：AV（即，多模态）语音源分离[10]。该领域的活动包括AV一致性的强大建模[12]，[15]；以及使用独立分量分析（ICA）或TF mask的AV一致性融合[16]；使用AV一致性解决BSS中的歧义[15]，[17]；利用视觉信息检测语音活动[18]，[19]；利用AV数据中的冗余来设计基于稀疏表示的有效语音分离算法[16]，[20]；最近，AV场景分析解决了语音分离中移动源[5]，[21]或混响时间长的环境[22]的难题。

显然已经开发出许多不同的方法来使用音频和视觉形式来解决语音源分离问题。为了以连贯的方式呈现这些内容，本教程的其余部分根据逐渐复杂的方式进行组织，如表1所示，视频被使用帮助语音源增强。这些方法的优势和劣势被突出显示在表格中，并将参考文献添加到论文中，在这些论文中可以找到实验研究的完整细节，这些实验研究显示了通过在处理过程中添加视频可以实现的性能增益。

[Figure 1]来自[9]的维纳滤波器的AV估计。LPC方法用于建模带语音。将基于LPC逆滤波频谱的音频特征与视觉特征（如嘴唇宽度（LW）和高度（LH））融合在一起，以增强嘈杂语音的LPCs频谱。因此，可以基于该LPC增强滤波器和从噪声语音的逆滤波获得的残余信号来获得增强语音信号。

[Table 1]一个针对语音增强和分离的AV方法的概论。方法是根据视频帮助语音源分离方面的不断增加的复杂性进行了分类的：从帧总索引（第二章）到合并的AV模态（第四章）包括可视场景分析（第三章），请参考以下详细信息，以了解详细的性能

## Methods based on visual voice activity

对音频和视频信号之间的联系进行建模的一种非常简单的方法是利用时域语音信号的语音活动。确实，在自然说话期间会出现停顿：例如，在呼吸期间或在爆破音之前（例如/p/）。可以通过嘴唇的运动部分预测这种静音[19]。基于这个想法，已经开发了几种纯粹基于视频的语音活动检测器（V-VAD）[29]：它们通常基于面部特征的速度，通常是嘴唇的运动。与音频VAD相比，此类V-VAD的主要优势在于它们不会被并发音频源（例如环境噪声或其他说话者）破坏。值得注意的是，此类模型并非旨在链接音频和视频功能，但是它们试图推断出在来自视频的音频模态中有关静音的非常粗略的信息[即，存在语音的概率 $P(s_{i}(t) \neq 0 | \zeta_{i}^{v}(t))$]或相反[即， $P(s_{i}(t) = 0 | \zeta_{i}^{v}(t))$]，其中$\zeta_{i}^{v}(t)$是与第$i$个源$s_i(t)$相关联的视觉信号。示例的利用V-VAD的语音增强方法将在接下来被讨论。

## Spectral Subtraction

一种简单的方法是通过嵌入视觉特征[23]，[24]来扩展经典谱减法：增强信号的谱表示为$|\hat{s}(t)|^2=|x(m,f)|^2-\alpha|d(m,f)|^2$其中$x(m,f)$是所测量的麦克风信号$x(t)$的STFT，$d(m,f)$是估计的干扰噪声频谱，并且$\alpha$是用于调整减法级别的参数。干扰噪声$d(m,f)$的频谱是从与目标源相关的静音窗口（即集合$\tau_i={t|p(s_i(t)=0|\zeta_i^v(t))}$）估计的。这些窗口可以通过V-VAD被有效地检测到，因此不会受到干扰音频噪声的破坏。

## AV postprocessing of audio ica

这种高级信息（语音/非语音帧）的另一种用法是将其嵌入有效的ICA框架中。在这种方法中，视觉信息在应用音频ICA算法后用作后处理。频域源分离通常在每个频点处受到排列不确定性的困扰：ICA框架允许将源恢复到全局排列（即，估计源的顺序是任意的）。因此，要恢复源，必须解决此问题（即，对所有$f$,置换矩阵$P(f)$必须相同）。一种非常直观有效的方法是估算与输出功率相关的置换[17]。实际上，V-VAD提供了一个二进制指示器，可显示特定说话者何时静音。然后，使用这些信息，可以通过在这些帧期间简单地最小化目标源的功率来解决排列不确定性。因此，该方法以非常最小的方式利用了AV依赖性，即AV特征的联合分布，但是已经表明，它消除了几乎所有的排列歧义。与纯音频方法相比，这需要相对较低的计算成本来减轻置换歧义，并允许仅提取特定语音源，而不是尝试恢复所有源。

## AV extraction based on temporal speech activity

这种高级信息（语音/非语音帧）的更有效利用是将其直接合并到分离标准[18]中以提取特定的说话人，从而提供比ICA方法更低的计算成本。确实，考虑一组时间样本$\mathcal{D}$，以便可以将信号源分为静默的$(\forall i\in\mathcal{S}_{\text{silent}}, \forall t\in\mathcal{D},s_{i}(t)=0)$和活动的$(\forall i\in\mathcal{S}_{\text{active}},\forall t\in\mathcal{D},s_{i}(t)=0)$，基于两个协方差矩阵的广义特征分解的纯音频代数方法，可以确定 1)静音说话者的数量（即$\mathcal{S}_{\text{silent}}$的基数）2)关联的支持子空间（即$\mathcal{S}_{\text{active}}$所覆盖的子空间）。换句话说，考虑到任何时间样本，包括一些不在D中的样本（即，$\mathcal{S}_{\text{silent}}$中的源可以变为活动状态），音频记录$x(t)$在后者标识的子空间上的投影将抵消$\mathcal{S}_{\text{active}}$中的所有源，而$\mathcal{S}_{\text{silent}}$中的源将维持不变。但是，该方法无法识别哪个声源是无声的，因此无法用于提取特定的讲话者。为了克服这个问题，可以使用加权内核主成分分析来改进此方法，其中对于一个专门的说话者，权重是音频无声概率（由特征值给出）和V-VAD提供的视频无声概率之间的混合[18]。这个简单的属性提供了一种非常有效且优雅的AV方法来提取语音源。

## VisuaL scene analysis-based methods

[Figure 2]基于视觉场景分析的语音增强方法的框图。视频定位基于面部和头部检测。视频跟踪器基于MCMC-PF，用于跟踪多个人。视频处理的输出是位置，到达方向和/或速度信息。根据视觉场景，通过独立于数据的波束形成器或智能初始化的视频辅助源分离方法来分离预处理的音频混合。最后，应用后处理来增强分离的音频源。

在上一节中，AV提取方法以非常粗糙的方式使用视觉模态：定义特定说话者是否沉默的简单二进制信息。在本节中，通过视觉分析场景以增强语音，从而更深入地使用了这种额外的模态[25]，[26]。这样的视觉场景分析为音源分离算法提供了说话人的位置信息，尤其是在处理移动音源，这是一个更具挑战性的问题时，因为混合滤波器现在是时变的。因此，由于需要精确估计混合物统计量的大量时间样本，因此经典ICA框架可能无效。这些方法分两个阶段实施：主要是基于多人跟踪（MHT）的视频场景分析（VSA）来估计房间或封闭环境中人员的位置，到达方向（DOA）和速度信息；和取决于场景的音频源的分离，如图2的示意图所示。

## Video processing for MHT

基于视频的面部和头部检测被应用于从单个图像进行的多人观察作为初始化。然后，将基于马尔可夫链蒙特卡罗的粒子滤波器（MCMC-PF）用于视频中的MHT。在[5]中提供了概率MHT的三个重要部分的更多细节—状态模型，测量模型和采样机制。与“基于可视语音活动的方法”一节中描述的通常在房间或封闭的环境中不可用的V-VAD相反，突出显示的是说话人面部的全正面特写视图，这些跟踪器不需要封闭的环境。上面提到的MHT方法为音源分离的AV场景建模提供了一个很好的框架：基于视频的跟踪器的输出是每个说话人的三维（3-D）位置，到达麦克风阵列中心的高度角$\theta_p$和方位角$\beta_p$。然后可以针对频率单元f和感兴趣的源$p=1,\ldots,P$来计算直接路径权重矢量$d_p(f,\theta_p,\beta_p)$以及可以在AV源分离方案中使用的速度信息。

## AV source separation of moving sources

语音源分离在处理移动源时是一个具有挑战性的问题[26]。在AV上下文中提出的特定说话者的提取取决于该说话者的速度。

### Physically moving sources

在VSA之后，如果人们在移动，则分离各个音频源的挑战在于混合滤波器随时间变化；这样，解混滤波器也应随时间变化，但是仅从音频测量中很难确定这些滤波器。在[21]中，已经开发了一种基于VSA的移动源语音分离的多阶段方法。此方法包括几个阶段，包括基于视频处理的语音源的DOA跟踪，基于由DOA生成的波束模式的波束成形对源的分离以及TF mask作为后处理。如上所述，可以从视频信号中获取直接路径参数矢量$d_p$，然后将其用于鲁棒最小二乘频率不变数据独立（RLSFIDI）波束形成器的设计，以分离音频源。TF mask用作后处理，可通过将干扰降低到更低的水平来进一步提高波束形成器的分离质量。但是，由于在某些T-F点对掩码的估计不准确，因此此类随时间变化的滤波技术可能会引入音乐噪声。为了克服这个问题，可以像[21]中那样使用倒频谱平滑等平滑技术。

### Physically stationary sources

经过视频处理后，如果判断说话人至少两秒钟处于静止状态，则直接路径参数矢量$d_p$和从音频混合中获得的白化矩阵将用于智能初始化学习算法，例如FastICA/IVA（许多学习算法对初始化敏感）[3]，[5]，它解决了ICA中的固有置换问题或IVA算法中的块置换，并提高了收敛性[27]。

[Figure 3]语音记录是从两个麦克风获得的。直接路径参数向量是在摄像机的帮助下计算的。ILD，IPD和利用直接路径参数向量的混合向量用于通过EM算法估计模型参数。由所得概率模型形成的最终概率mask用于源分离。

## T-F masking based on VSA

最近，已经提出了一种基于视频辅助模型的用于混响时间很长的不确定情况下的源分离技术，[22]。这种概率性T-F掩蔽方法受计算听觉场景分析（CASA）和BSS的推动，后者依赖于信号稀疏的假设（图3）。如图[30]所示，对听觉水平差分（ILD），听觉相位差分（IPD）和混合向量进行建模，并且从视频处理中获得的直接路径参数向量$d_p$用作混合向量的均值参数。使用EM算法迭代更新参数。由于EM算法也对初始化敏感，因此我们使用从视频处理获得的说话人的位置信息来初始化方向矢量参数。

为了为每个静态源$s_i(t)$形成AV概率TF mask$\mathcal{M}^{av}_i(m,f)$，使用IPD和ILD模型以及混合矢量模型，这些模型利用了借助视频获得的直接路径权重矢量。这是一个隐藏的最大似然参数估计问题，因此EM算法可以提供解决方案。可以在[22]中找到广泛的评估，这些评估证实了利用视觉模式分析场景的优势。

## Full joint AV Modeling-based Methods

然后，使用多模态的最复杂的方法是建立完整的语音AV模型，而不是V-VAD的二进制建模（请参见“基于可视语音活动的方法”一节）或VSA（请参见“基于视觉场景分析的方法”）。本节介绍了这些模型中的两个及其在语音提取中的用途：1）AV统计模型，以及2）基于AV词典学习（DL）的稀疏表示模型。使用统计建模方法，通常在特征空间上明确建立AV一致性，从而在AV信号的所有观察帧中提供整体表示[10]，[11]，[28]，[31]。另一方面，使用基于稀疏表示的方法，通过将AV信号分解为从字典中选择的少量信号分量（即原子）的线性组合，可以隐式建模AV一致性[16]，[32]。稀疏模型已显示出在捕获局部信息（例如AV信号的时间动态结构）方面有效，而这些信息可能会在统计建模方法中丢失，但对于语音感知可能至关重要。请注意，我们从建模和优化算法的角度来区分这两个模型，而不是信号的特性，因为稀疏度可以被视为信号的统计特性。例如，如果稀疏模型建立在某些统计模型描述的特征空间上，则可以同时使用这两个模型。

## Statistical AV-based methods

### AV model

音频和视觉模态之间的一致性可以通过例如高斯混合模型（GMM）进行联合建模，其中一致性被表示为联合AV概率密度函数（AV-PDF）:
$$
p_{1}^{\mathrm{av}}(\zeta^{a}(m), \zeta^{v}(m))=\sum_{k=1}^{K} w_{k} p_{G}(\zeta^{a}(m), \zeta^{v}(m) | \mu_{k}^{\mathrm{av}}, \Sigma_{k}^{\mathrm{av}}) \tag{1}
$$
其中上标$a$和$v$分别指音频和视觉模态，$\zeta^{a}(m)$和$\zeta^{v}(m)$分别是第$m$帧的音频和视觉观察向量；$\mu_k^{av}$和$\Sigma_{k}^{av}$是第$k$个高斯核的均值矢量和由其概率密度函数（PDF）$p(\odot|\mu,\Sigma)$定义的协方差矩阵，$w_k$是相关核的权重，$K$是混合项的数量。（为简化开发，我们将使用相同的符号表示AV特征向量和AV序列。）通常，$\zeta^{a}(m)$可以选择作为音频特征向量，例如傅立叶变换或Mel频率倒谱系数的模数的帧索引为m的窗口帧信号的[33]，而$\zeta^{v}(m)$是视觉特征矢量，包含一些形状参数，例如嘴唇的宽度和高度或基于外观的有效视觉特征[34]。在频域中处理对数标度音频参数时，更合适的模型是Log–Rayleigh PDF，因为该PDF明确地对数标度的非对称属性建模。因此，AV-PDF可以表示为[31]：
$$
p_{2}^{\mathrm{av}}(\zeta^{a}(m), \zeta^{v}(m))=\sum_{k=1}^{K} w_{k} p_{LR}(\zeta^{a}(m) | \Gamma_{k}^{a}) p_{G}(\zeta^{v}(m) | \mu_{k}^{v}, \Sigma_{k}^{v}) \tag{2}
$$
其中$p_{LR}(\zeta^{a}(m)|\Gamma_{k}^{a})$是局部或功率系数是由对角线元素$\Gamma_{k}^a$定义的Log-Rayleigh PDF（更多信息，请参见[31]）。这样的AV-PDF不仅可以共同为这两种模态建模，而且还可以考虑到语音的歧义性（即，相同形状的嘴唇可以产生多种声音，例如法语中的/u/和/y/）。通常使用EM算法从干净的训练AV数据库中获取AV-PDF参数。

### Extraction by direct AV criteria

视音源分离的第一种方法[10]，[28]是基于最大化（1）中描述的AV-PDF模型的AV一致性：
$$
\hat{\mathbf{b}}=\arg \max_{\mathbf{b}} p_{1}^{a v}(\mathbf{b}^{T} \mathbf{x}(t), \zeta^{v}(t)) \tag{3}
$$
其中$\mathbf{b}$是瞬时情况下特定说话人的提取向量，上标$\cdot^T$表示转置运算符。即使在处理简单的元音和辅音序列时这种方法被证明是有效的[10]，该方法也有两个重要的缺点：1）对于自然语音，很难获得相关的AV概率模型；以及2）当考虑混响环境时，由于分离滤波器的尺寸，直接最大化AV-PDF的计算效率很快变低。

另一方面，ICA [1]是一个非常有效的框架，可以将源与几种混合物分开。结果，通过定义AV惩罚的ICA准则：$\{\hat{B}(f)\}_f=\argmin_{\{\hat{B}(f)\}_f}J_{ICA}(\{\hat{B}(f)\}_f)+P_{AV}(p_1^{av})$[11]，自然可以将AV约束嵌入到更经典的频域ICA准则$J_{ICA}(\{\hat{B}(f)\}_f)$中其中约束项$P_{AV}(\cdot)$是（1）中AV-PDF的函数。请注意，我们有意使$J_{ICA}(\{\hat{B}(f)\}_f)$保持通用，因为可以使用文献中定义的许多频域ICA标准，例如[1]。可以看到，此标准是在基于ICA的估计源的统计独立性（第一项）与估计源和视频特征的AV一致性（第二项）之间的权衡。与纯音频标准相比，此AV约束仅稍微改善了信号干扰（SIR）比[11]：这主要是由于难以提出相关的AV-PDF和适当的AV约束。

### AV postprocessing of audio ICA

一种自然的方法是通过最大化$p_2^{av}(\cdot,\cdot)$（2）定义的AV一致性来估计全局排列。然而，即使这些算法被证明对解决排列歧义是有效的，它们仍然遭受计算成本和难以准确训练代表自然语音所有特征的统计参数的困扰。

## Sparse modeling

尽管以前的方法似乎是很自然的AV提取方法，但从“全局”的角度来看，这些方法中使用的AV一致性通常是在特征空间中对所有AV数据帧进行建模的。由于难以训练相关的AV统计模型，这些方法经常不能提供准确的音频信号估计。为了解决这个局限性，在[16]和[32]中考虑了使用基于DL的稀疏近似（称为稀疏建模）来捕获AV相干的另一种方法。正如[16]中指出的那样，该技术可以捕获“局部”信息，即相邻样本之间的互连，这对于在嘈杂环境中的语音感知非常重要。

### Sparse coding of AV signals and DL of the AV atoms based on a generative AV model

[Figure 4]人工生成的示意图，用于显示使用生成模型将AV序列表示为少量（在此情况下为两个）原子的线性组合。音频序列（频谱图）显示在（a）中，视频序列（一系列图像帧，用实线表示为矩形）显示在（b）中。模式A，B和C以及点分别对应于两个视觉原子。如点划线的矩形所突出显示的，序列中的AV一致部分是通过在两个位置缩放和分配原子来表示的。音频流以对数刻度显示。音频原子是随机生成的频谱图模式，而不是现实的音素或语音单词。该图是对[16]的修改，可以从真实的AV语音数据中找到AV序列和AV原子的示例。

为了获得AV信号的稀疏表示，可以使用生成模型[16]，[32]，其中，AV序列$\zeta=(\zeta^a,\zeta^v)$由过剩字典$\mathcal{D}=\{\phi_{k}\}_{k=1}^{K}$中的少量AV原子$\phi_{k}=(\phi^a_{k},\phi^v_{k})$来描述，其中，为便于记述，此处省略了离散时间索引$t$。音频原子$\phi^a_{k}$通常是音频信号分量的STFT的对数模量，而视频原子$\phi^v_{k}$是视频信号的嘴巴区域（即图像帧中嘴巴所在的区域）。我们使用示意图来解释AV序列和AV原子之间的关系，如图4所示，其中每个音频原子与相应的视频原子串联出现在相关的时空（TS）位置视频。在此示例中，AV序列仅由两个AV原子表示，在特定的TS位置，两者之间有些重叠。

给定一个AV信号和一个字典$\mathcal{D}$，编码处理旨在找到一个稀疏系数集，该稀疏系数集会根据匹配准则对原始序列进行合适的近似。这可以通过许多算法来实现，包括贪婪算法，例如众所周知的匹配追踪（MP）或正交MP算法。在[32]中，MP算法已扩展到AV-MP版本以获得编码系数，其中匹配标准定义为第n次迭代的AV序列$(R^n\zeta)$的残差与转录的AV原子$\phi_k$之间的内积$\langle,\rangle$：
$$
J_{1}^{a v}(R^{n} \zeta, \phi_{k})=|\langle R^{n} \zeta^{a}, \mathcal{T}_{\breve{m}}^{a} \phi_{k}^{a}\rangle|+|\langle R^{n} \zeta^{v}, \mathcal{T}_{i j ; \breve{l}}^{v} \phi_{k}^{v}\rangle| \tag{4}
$$
其中$\mathcal{T}_{\breve{m}}^a$是音频原子的时间转换算子（即，将音频原子移动$\breve{m}$个时间帧），而$\mathcal{T}_{i j ; \breve{l}}^{v}$是视频原子的时间空间转换算子（即，将视频原子沿时间轴移动$\breve{l}$个时间帧和沿图像帧的水平和垂直轴移动$(i,j)$个像素）。但是，如[16]所示，由于两种模态之间的不平衡（由于比例差异），后一种匹配准则可能会导致单模态准则。因此，在[16]中提出了以下标准：
$$
J_{2}^{a v}(R^{n} \zeta, \phi_{k})=|\langle R^{n} \zeta^{a}, \mathcal{T}_{\breve{m}}^{a} \phi_{k}^{a}\rangle| \times \exp \{\frac{-1}{I JL}\|R^{n} \zeta^{v}-\mathcal{T}_{i j, \breve{l}}^{v} \phi_{k}^{v}\|_{1}\} \tag{5}
$$
其中$I$和$J$分别是视频原子$\phi_k^v$的宽度和高度像素数，$L$表示持续时间；$\|\cdot\|_1$为$\ell_1$范数。

学习过程是适应$K$个字典原子$\phi_{k\in\{1,\ldots,K\}}$以适应训练AV序列。几种众所周知的DL算法可用于此目的，例如奇异值分解（K-SVD）[35]。在[16]中，在每次迭代中分别使用K-SVD和K-means算法分别更新音频和视觉原子，以考虑在这两种模态上实施的不同稀疏性约束。稀疏编码和DL阶段通常以交替方式执行，直到优化预定义标准（如（5））为止。

### Sparse AV-DL-based AV speech separation

根据AV-DL方法，提出了基于T-F mask的BSS方法[16]，其中，将纯音频算法[36]生成的音频T-F mask$\mathcal{M}^a(m,f)$与通过幂律变换从视觉模态中定义的mask$\mathcal{M}^v(m,f)$凭经验融合以定义AV T-F mask：
$$
\mathcal{M}^{av}(m,f)=\mathcal{M}^a(m,f)^{r\mathcal{M}^v(m,f)} \tag{6}
$$
其中功率系数r是通过基于视觉信息在确定混合物中每个T-F点的源占据可能性时有多大信心而对$\mathcal{M}^v(m,f)$应用非线性映射函数而获得的[16]。有多种方法可用于融合音频和视觉mask，例如这两个mask的简单线性组合。然而，与幂律变换相比，这种简单的方案在考虑视觉信息的置信度方面效果不佳（更多关于使用幂律变换的动机的讨论可以在[16]中找到）。从视频定义的mask可以通过以下方式获得：
$$
\mathcal{M}^{v}(m, f)=\left\{\begin{array}{ll}
1, & \text { if } \hat{\zeta}^{a}(m, f)>\zeta^{a}(m, f) \\
\hat{\zeta}^{a}(m, f) / \zeta^{a}(m, f), & \text { otherwise }
\end{array}\right. \tag{7}
$$

这里，$\hat{\zeta}^a(m,f)$是通过将混合物（连同视觉序列）映射到AV词典上而从语音混合中重建的音频信号。请注意，即使从仅音频序列$\zeta^a(m,f)$和$\hat{\zeta}^a(m,f)$定义了后一个mask，由于$\hat{\zeta}^a$取自$(\hat{\zeta}^a(m,f),\hat{\zeta}^v(,x,l))$，它表示新AV序列$\zeta=(\zeta^a;\zeta^v)$的AV近似值，所以我们依然认为后一个mask收到了视觉的启发。换句话说，$\hat{\zeta}^a(m,f)$是从AV序列在AV字典$\mathbb{D}$上稀疏分解获得的AV序列$\zeta$的音频信号的最佳估计。最后，可以将抗噪AV mask$\mathcal{M}^{av}(m,f)$应用于混合物的TF频谱，以进行目标语音分离。图5显示了与理想二进制掩码（IBM）相比的$\mathcal{M}^{v}(m,f)$，$\mathcal{M}^{v}(m,f)$和AV掩码$\mathcal{M}^{v}(m,f)^{r(\mathcal{M}^{v}(m,f))}$的示例。可以看出，融合的AV mask提高了音频mask的质量和视觉mask的分辨率。在[16]中，表明幂律变换的性能优于平均运算，即$(\mathcal{M}^a+\mathcal{M}^v)/2$。

## Conclustons and Future directions

在过去的十年中，AV语音源分离已成为信号处理研究中特别有趣的领域。它旨在通过使用视频信息，从而模仿人类的多模态方法，来改进用于语音提取的经典BSS方法。如本文所示，语音的双模态可用于不同的复杂程度，以帮助分离音频源：从非常粗糙的二进制信息到完整的AV模型，或者从简单的联合唇形参数到依赖于数据的声学AV词典中表示的功能。结果，使用各种级别信息的方法表现出不同的优势和劣势，如表1所示。使用视频中的额外信息的主要优点是解决仅音频算法无法轻松解决的问题：在强烈混响的环境中处理背景噪声和干扰，以及多个可能移动的声源。

有许多进一步研究的方向。基于统计方法的AV一致性需要高质量，低维的特征以进行准确且计算高效的建模，因此可以利用来自流形或深度学习的新兴方法。由于稀疏编码算法中需要大量的数值运算，AV DL中试图捕获双模态数据中AV信息结构的当前方法在计算上是昂贵的。低复杂度和鲁棒性算法是非常需要的，需要加以开发。而且，要嵌入到诸如智能手机之类的日常设备中，必须提出实时方法来克服许多当前算法的批处理性质。从长远来看，在脑科学和心理学领域的基础上建立更丰富的利用心理-视觉特性的模型可以潜在地进一步改善视听语音分离系统，但这为该领域的未来研究提出了特殊的挑战。

最后，由于语音源分离显然是从源的双模态中获利的，因此还应使用多模态数据探索源分离/提取的其他领域，例如脑成像，它可以通过脑电图，脑磁图，磁记录成像和正电子发射断层扫描来记录大脑活动。下一代智能多模态信号处理技术将结合这些信息，以提供基于单模态数据的方法无法实现的根本改善的性能。

[Figure 5]视觉mask，音频mask，AVmask（幂律）和IBM之间的比较，显示了AVmask的清晰度得到改善。对于IBM，零用黑色表示，一个用白色表示。尽管视觉蒙版看起来很平滑，但是缺少一些详细的音频信息。通过将这些mask与IBM进行比较，可以观察到AV mask可提供最佳效果。该图是对[16]的修改，其中可以找到更多的定量比较和分析。
