# Visual Speech Enhancement

## Abstract

在嘈杂的环境中拍摄视频时，可以通过可见的嘴部动作来增强视频中说话者的声音，从而减少背景噪音。虽然大多数现有方法仅使用音频输入，但基于视听神经网络的视觉语音增强可提高性能。
我们在训练数据中包含视频，并在其中添加了目标说话者的声音作为背景噪声。由于音频输入不足以将讲话者的语音与他自己的声音区分开，因此经过训练的模型可以更好地利用视觉输入并将其很好地推广到不同的噪声类型。
所提出的模型在两个公共唇读数据集上优于先前的视听方法。它也是第一个在非唇读设计的数据集上展示的方法，例如巴拉克·奥巴马每周公开演讲。

## 1. Introduction

语音增强旨在提高在嘈杂环境中录制音频时的语音质量和清晰度。应用包括电话对话，视频会议，电视报道等等。语音增强功能还可用于助听器[1]，语音识别和说话人识别[2，3]。语音增强一直是广泛研究的主题[4，5，6]，并且最近得益于机器唇读[7，8]和语音阅读[9，10，11]的进步。
我们提出了视听端到端端经网络模型，用于将可见说话者的语音与背景噪声分开。一旦在特定的说话者上训练了模型，就可以用来增强该说话者的声音。我们假设有一个视频显示目标讲话者的面部以及嘈杂的音轨，并使用可见的嘴巴动作将所需的声音与背景噪声隔离开。
虽然训练深度神经网络以区分不同来源的独特语音或听觉特征的想法在某些情况下可能非常有效，但性能受到源差异的限制，如图[12、13]所示。我们表明，使用视觉信息可以在不同情况下显着改善增强性能。为了涵盖仅使用音频信息无法完全分离目标语音和背景语音的情况，我们在训练数据视频中添加了从目标说话人的声音中提取的合成背景噪声。通过训练数据中的此类视频，受过训练的模型可以利用视觉输入并将其很好地推广到不同的噪声类型。
我们在不同的语音增强实验中评估模型的性能。首先我们展示在两个公共数据集上与之前的基线相比更好的性能：GRID语料库[14]和TCD-TIMIT [15]，都被设计用于视听语音识别和唇读。我们还将在巴拉克·奥巴马（Barack Obama）每周公开演讲中展示语音增强功能。

### 1.1 Related work

传统的语音增强方法包括频谱恢复[16，5]，维纳滤波[17]和基于统计模型的方法[18]。最近，深度神经网络已被用于语音增强[19，20，12]，通常优于传统方法[21]。

#### 1.1.1 Audio-only deep learning based speech enhancement

用于单通道语音增强的先前方法主要使用仅音频输入。 Luet等人[19]训练用于对语音信号进行消噪的深度自动编码器。他们的模型预测表示干净语音的mel声谱图。 pascualetal。[22]使用生成对抗网络并在波形级别进行操作。通过训练一个深度神经网络来区分不同源的独特语音特征使得同时分离几个人的语音混合成为可能。例如频带，音高和chirps，如[12、13]所示。尽管总体表现不错，但仅音频方法在分离相似的人类声音时仍会降低性能，这在相同性别的混合中通常会看到。

#### 1.1.2.  Visually-derived speech and sound generation

存在从说话者的无声视频帧中产生可理解语音的不同方法[11、10、9]。在[10]中，Ephratet等人从说话人的无声视频帧序列中产生语音。他们的模型使用视频帧和相应的视觉流来输出表示语音的频谱图。欧文塞特[23]使用递归神经网络来预测人们用静音鼓槌敲击和刮擦物体的无声视频中的声音。

#### 1.1.3.  Audio-visual multi-modal learning

视听语音处理的最新研究广泛地使用了神经网络。 Ngiamet等人的工作[24]这是这方面的开创性工作。他们演示了跨模态特征学习，并表明如果在特征学习时间同时出现音频和视频，则可以学习一种模态的更好特征（例如视频）。具有视听输入的多模态神经网络也已用于唇读[7]，唇同步[25]和鲁棒的语音识别[26]。

Figure 1：我们的encoder-decoder模型架构的示意图。将以嘴巴区域为中心的5个视频帧序列馈送到卷积神经网络中，以创建视频编码。相应的嘈杂语音的声谱图以类似的方式编码为音频编码。通过级联视频和音频编码，可以获取单个共享嵌入，并将其馈送到3个连续的完全连接的层中。最终，使用音频解码器对增强语音的频谱图进行解码。

#### 1.1.4.  Audio-visual speech enhancemen

视听语音的增强和分离也已经完成[27]。 Kahn和Milner [28，29]使用手工制作的视觉特征来导出用于说话人分离的二进制和软mask。 Houet等人[30]提出了卷积神经网络模型以增强语音噪声。他们的网络获得一系列裁剪到说话者嘴唇区域的帧，以及代表嘈杂语音的声谱图，并输出代表增强语音的声谱图。 Gabbayet等人[31]将视频帧输入经过训练的语音生成网络[10]，并使用预测语音的频谱图构建用于将纯净语音与嘈杂输入分离的mask。

## 2. Neural Network Architecture

语音增强神经网络模型得到两个输入：（i）一序列显示发言人嘴巴的视频帧；（ii）嘈杂音频的声谱图。输出是增强语音的频谱图。网络层以encoder-decoder方式堆叠（图1）。编码器模块由一个双塔式卷积神经网络组成，该网络接收视频和音频输入并将它们编码为表示视听特征的共享嵌入。解码器模块由转置的卷积层组成，并将共享嵌入解码成表示增强语音的频谱图。整个模型是端到端训练的。

### 2.1. Video encoder

视频编码器的输入是一序列5个连续的灰度视频帧，大小为128×128，裁剪并居中于嘴巴区域。使用5帧效果很好，其他数量的帧也可能work。视频编码器具有表1中描述的6个连续的卷积层，每层后面是批处理归一化，非线性的Leaky-ReLU，max pooling和0.25的dropout。

### 2.2. Audio encoder

输入和输出音频均由对数mel声谱图表示，该图具有80个频率区间（介于0到8kHz之间）和20个跨越200 ms的时间步长。

Table 1：视频和音频编码器的结构细节，视频编码器中使用的pooling size和stride始终为2×2，用于所有六层。

像以前在几个音频编码网络中所做的一样[32，33]，我们将音频编码器设计为使用频谱图作为输入的卷积神经网络。该网络由5个卷积层组成，如表1所示。每个层后面是批处理归一化和Leaky-ReLU，以实现非线性。为了保持时间顺序，我们使用跨步卷积而不是max pooling。

### 2.3. Shared representation

视频编码器输出具有2,048个值的特征向量，音频编码器输出3,200个值的特征向量。这些特征向量被连接到一个共享的embedding中，该embedding体现了视听特征，具有5,248个值。然后将共享的embedding馈入3个连续的全连接层的块中，分别具有1,312、1312和3200的大小，然后将所得矢量馈入音频解码器。

### 2.4. Audio decoder

音频解码器由5个转置的卷积层组成，可镜像音频编码器的各层。最后一层的大小与输入频谱图相同，表示增强的语音。

### 2.5. Optimization

对网络进行训练，以最大程度地减少输出频谱图和目标语音频谱图之间的均方误差（l2）损失。我们使用初始学习率为5e-4的Adam优化器进行反向传播。一旦学习停滞，即5个epoch内验证错误没有改善，学习率将降低50％。

## 3. Multi-Modal Training

具有多模态输入的神经网络通常可以由输入之一控制[34]。在先前的工作中已经考虑了不同的方法来克服这个问题。 Ngiamet等人[24]建议在训练过程中偶尔将其中一种输入模态（例如视频）清零，而仅使用另一种输入模态（例如音频）。这个想法已在唇读[7]和语音增强[30]中采用。为了强制使用视频功能，Houet等人[30]添加应该类似于输入的辅助视频输出。

我们通过引入新的训练策略来强制开发视觉功能。我们在训练数据样本中添加了噪声，即噪声是同一说话者的声音。由于仅使用音频信息就不可能将同一人说的两个重叠句子分开，因此除了音频功能外，网络还必须利用视觉功能。我们表明，使用这种方法训练的模型可以很好地概括不同的噪声类型，并且能够将目标语音与无法区分的背景语音区分开。

## 4. Implementation Details

### 4.1. Video pre-processing

在我们所有的实验中，视频均重新采样为25 fps。将视频分为每个5帧（200毫秒）的非重叠段。在每帧中，我们使用[35]提出的68个面部标志中的20个嘴部标志，裁剪出一个以口为中心的窗口，尺寸为128×128像素。因此，用作输入的视频片段的大小为128×128×5。我们通过减去主题均值帧并除以标准差来对整个训练数据上的视频输入进行归一化。

### 4.2. Audio pre-processing

相应的音频信号被重新采样到16 kHz。短时傅立叶变换（STFT）应用于信号波形。频谱图（STFT幅度）用作神经网络的输入，并且保留相位以重建增强信号。我们将STFT窗口大小设置为640个样本，等于40毫秒，对应于单个视频帧的长度。我们一次将窗口移动160个样本，从而产生75％的重叠。对数梅尔尺度谱图是通过将谱图乘以梅尔间隔的滤波器组来计算的。对数mel音阶频谱图包含80个mel频率（从0到8000 Hz）。我们将频谱图切成200毫秒长的片段，对应于5个视频帧的长度，从而得到大小为80×20的频谱图：20个时间样本，每个样本具有80个频率单元。

### 4.3. Audio post-processing

语音帧被一一推断，并被连接在一起以创建完整的增强频谱图。然后通过将mel谱图乘以mel滤波器组的伪逆来重构波形，然后应用逆STFT。我们使用有噪声的输入信号的原始相位。

图2：来自GRID（左上），TCD-TIMIT（右上），说普通话的人（左下）和Obama（右下）数据集的样本帧。红色的边界框标记以嘴为中心的作物区域。奥巴马的视频有各种各样的背景，照明，分辨率和亮度。

## 5. Datasets

来自数据集的样本帧如图2所示。

### 5.1. GRID Corpus and TCD-TIMIT

我们对GRID视听句子语料库[14]的说话人进行了实验，GRID视听句子语料库是一个庞大的音频和视频（面部）录音数据集，包含34个人（18位男性，16位女性）所说的1000个句子。我们还对TCD-TIMIT数据集进行了实验[15]，该数据集由60位自愿讲者组成，每人约200个视频，还有3位口语者，这些人经过专门培训，可以帮助聋人理解他们的视觉语音。说话者被记录说出TIMIT数据集中的各种句子[36]。

### 5.2. Mandarin Sentences Corpus

Houet等人[30]准备了一个视听数据集，其中包含以普通话为母语的人讲的320句普通话的视频录制。每个句子包含10个汉字，其音素旨在平均分配。每个发音的长度约为3-4秒。将录音记录在一个光线充足的安静房间中，并从正面观看说话者。

### 5.3. Obama Weekly Addresses

与专门为唇读而准备的数据集相比，我们在更一般的条件下评估了模型的性能。为此，我们使用包含巴拉克·奥巴马（Barack Obama）给出的每周地址的数据集。该数据集包含300个视频，每个视频时长2-3分钟。数据集在比例（缩放），背景，光线和面部角度以及音频录制条件方面差异很大，并且包含无限制的词汇。

## 6. Experiments

我们使用本节中提到的四个数据集在几个语音增强任务上评估我们的模型。 5.在所有情况下，都从LibriSpeech [37]数据集中采样背景语音。对于环境噪声，我们使用不同类型的噪声，例如雨，摩托车发动机，篮球弹跳等。语音和噪声信号与SNR混合。训练和测试都为0 dB，但普通话实验中使用的做法与[30]相同。在每个样本中，目标语音与背景语音，环境噪声或目标说话者的其他语音混合在一起。我们称后者为self混合。
我们使用两个客观评分来报告评估结果：SNR用于衡量降噪效果，而PESQ用于评估语音质量的提高[38]。由于收听音频样本对于理解语音增强方法的有效性至关重要，因此可以在我们的项目网页上找到补充材料。

### 6.1. Baselines and previous work

我们通过训练具有相似体系结构的竞争性纯音频版本来展示使用视觉信息的有效性，该版本具有类似的体系结构（去除可视流）。训练此基线不会涉及self混合，因为在这种情况下纯音频分离是不适当的。可以看出，纯音频基准始终比我们的模型获得更低的性能，特别是在self混合中完全没有提高语音质量。为了验证我们的假设，即使用目标说话人的样本作为背景噪声使模型对不同噪声类型以及背景语音与目标语音无法区分的情况具有鲁棒性，我们再次在训练集中没有self混合的情况下再次训练模型。结果表明，尽管该模型可以访问视觉流，但它无法分离相同语音的语音样本。表2给出了详细的结果。
我们表明，我们的模型优于之前的Vid2speech [10]和Gabbayet等人[31]的工作。在GRID和TCD-TIMIT的两个视听数据集上进行分析，并在奥巴马的新数据集上实现了SNR和PESQ的显着改善。可以看出，在TCD-TIMIT数据集上，结果有些令人信服。一种可能的解释可能是与其他实验（40-60分钟）相比，训练数据中的清晰语音（20分钟）更小。在普通话实验中，我们遵循Houetal的协议[30]并在其建议的数据集上训练模型，这些样本混合了不同的SNR配置中的发动机环境噪声和其他干扰噪声类型。表3表明，我们的模型在其建议的测试集上获得了更好的性能，但应注意，PESQ并不准确关于中文[39]。

## 7. Concluding remarks

提出了端到端的神经网络模型，将可视说话者的声音与背景噪声分离开来。此外，提出了一种有效的视听语音增强训练策略-将同一个人说出的重叠句用作噪声。这样的训练建立了一个模型，该模型对于目标说话者和杂音说话者的相似声音特征具有鲁棒性，并且可以有效利用视觉信息。
所提出的模型不断提高了嘈杂语音的质量和可理解性，并且在两个公共基准数据集上均优于以前的方法。最后，我们首次展示了不是为唇读研究设计的通用数据集的视听语音增强功能。我们的模型紧凑，可在较短的语音段上运行，因此适合实时应用。平均而言，增强200毫秒的片段需要36毫秒的处理时间（使用NVIDIA Tesla M60 GPU）。
我们注意到，在缺少一种输入方式的情况下，我们的方法会失败，因为在训练过程中同时使用了音频和视频。
组合视听处理领域非常活跃。在相机准备就绪截止日期之前出现的近期工作显示出很大的进步，包括[40]，[41]，[42]，[43]。

Table2：我们对模型的评估，并与基线和以前的工作进行了比较。我们的模型在不同噪声类型上的降噪和语音质量均取得了显着改善。请参阅文本以进行进一步讨论。

Table3：我们的模型在普通话数据集上的评估，以及与基线和Houet等人[30]的比较，其中噪声类型是语音和环境。
