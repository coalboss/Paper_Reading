# Audio-Visual Speech Enhancement Using Multimodal Deep Convolutional Neural Networks

这是一篇来自于台湾中央研究院与国立台湾大学的研究者有关于音视频语音增强的技术类paper，发布时间2018年1月，被IEEE Transactions on Emerging Topics in Computational Intelligence接收。

## Abstract

语音增强（SE）旨在减少语音信号中的噪声。大多数SE技术只专注于处理音频信息。在这项工作中，受多模态学习的启发，该学习利用了来自不同模态的数据，以及卷积神经网络（CNN）在SE中的最新成功，我们提出了一种视听深度CNN（AVDCNN）SE模型，该模型将音频和视觉流合并为统一的网络模型。我们还提出了一种多任务学习框架，用于在输出层重建音频和视频信号。准确地说，建议的AVDCNN模型被构造为视听编码器-解码器网络，其中首先使用各个CNN处理音频和视频数据，然后将其融合到一个联合网络中以生成增强的语音（主要任务）并在输出层重建图像（次要任务）。以端到端的方式训练模型，并通过反向传播共同学习参数。我们使用五个标准评估增强型语音。结果表明，与仅基于音频的CNN的SE模型和两个常规的SE方法相比，AVDCNN模型具有显着优越的性能，证实了将视觉信息集成到SE过程中的有效性。此外，AVDCNN模型也优于现有的视听SE模型，这证明了其在SE中有效组合视听信息的能力。

## 1 Intrduction

语音增强(SE)的基本目标是通过减少被噪声退化语音中的噪声成分以提高带噪语音信号的质量和可理解性。为了获得令人满意的性能，SE已被用作各种与语音相关的应用程序中的基本单元，例如自动语音识别[1、2]，说话者识别[3、4]，语音编码[5、6]，助听器[ [7–9]和人工耳蜗[10–12]。在过去的几十年中，已经提出并证明了许多SE方法可以提供改进的声音质量。一种值得注意的方法是频谱恢复，估计增益函数（基于噪声和语音成分的统计数据），然后将其用于抑制频率域中的噪声成分，以从嘈杂的语音中获得干净的语音频谱[13] –17]。另一类方法采用非线性模型将有噪声的语音信号映射到纯净的[18-21]。近年来，基于深度学习的SE方法已经被提出并得到了广泛的研究，例如去噪自动编码器[22，23]。与传统的SE模型相比，使用深度神经网络（DNN）的SE方法通常表现出更好的性能[24-26]。利用递归神经网络（RNN）和长短期记忆（LSTM）模型的方法也已被证实具有良好的SE和相关语音信号处理性能[27-29]。此外，受卷积神经网络（CNN）图像识别成功的启发，基于CNN的模型因其在处理带噪声语音的像图像一样的2D时频表示方面[31,32]的优势而在SE中获得了良好的效果。

除语音信号外，视觉信息在人机或人机交互中也很重要。McGurk效应的研究[33]表明，嘴或嘴唇的形状可能在语音处理中起重要作用。因此，语音处理的许多领域都采用了视听多模态[34-39]。结果表明，与仅使用音频模态的视觉模态相比，视觉模态可增强语音处理性能。此外，[40，41]中已讨论了有关音频和视觉特征融合的主题，其中附加的可靠性措施是另一方面，在[42，43]中，基于神经网络的体系结构在多模态学习中采用了直观的融合方案。在音视频语音增强领域也有一些相关研究[44-50]。其中大多数是基于增强型滤波的，借助唇形信息中的手工视觉特征。最近，还提出了一些基于深度学习的视听SE模型[51，52]。在[51]中，梅尔滤波器组和Gauss-Newton可变形部分模型[53]用于提取音频和嘴形特征。实验结果表明，在几种标准化的评估中，具有视听输入的DNN优于仅具有音频输入的DNN。在[52]中，作者提出分别使用DNN和CNN处理音频和视频数据。在训练期间，将嘈杂的音频特征和相应的视频特征用作输入，并将音频特征用作目标。

在当前的工作中，我们采用了CNN来处理音频和视频流。这两个网络的输出融合为一个联合网络。嘈杂的语音和视觉数据放置在输入端，干净的语音和视觉数据放置在输出端。整个模型以端到端的方式进行训练，并构造为视听编码器-解码器网络。值得注意的是，在模型训练期间，输出层的视觉信息成为约束的一部分，因此系统采用了考虑异构信息的多任务学习方案。这种独特的视听编码器-解码器网络设计尚未在任何相关工作中使用[51，52]。简而言之，所提出的视听SE模型利用了CNN已被证明在语音增强方面是有效的的优势[31，32]和图像和面部识别[54、55]，用于音频和视频流，以及深度学习的属性，即通过在多模态学习任务中进行端到端学习和直观融合方案来减少人为工程的工作量。据我们所知，这是在基于深度学习的视听SE模型中同时利用所有上述属性的第一个模型。

我们的实验结果表明，在几个标准评估指标（包括对每个对象的感知评估）方面，所提出的视听SE模型优于四个基线模型，包括三个纯音频的SE模型和[51]中的视听SE模型。语音质量（PESQ）[56]，短时目标清晰度（STOI）[57]，语音失真指数（SDI）[58]，助听器语音质量指数（HASQI）[59]和助听器语音感知指数（HASPI）[60]。这证实了将视觉信息整合到基于CNN的多模态SE框架中的有效性，以及其将异类信息结合为视听SE模型的卓越效果。此外，基于我们的音频的替代融合方案（即早期融合）对视觉模型进行了评估，结果表明所提出的体系结构优于早期融合体系。

本文的其余部分安排如下：第二部分描述了音频和视频流的预处理。第三节介绍了针对SE提出的基于CNN的视听模型，并描述了四个基线模型进行比较。第四节描述了实验装置和结果，第五节进行了讨论。第六节介绍了本研究的结论。

## 2 Datasets and Preprocessing

在本节中，我们提供音频和视频流的数据集的详细信息以及预处理步骤。

### A Data Collection

准备好的数据集包含由母语人士说的320言普通话句子的视频记录。录制脚本基于台湾普通话听力测试（台湾MHINT）[61]，其中包含16个列表，每个列表包含20个句子。这些句子经过专门设计，在列表中具有类似的音素特征。每个句子都是唯一的，包含10个汉字。每个发声的长度约为3-4秒。话语记录在一个光线充足的安静房间中，并且从正视图拍摄了说话人。视频以每秒30帧（fps）的速度录制，分辨率为1920像素×1080像素。立体声音频通道以48 kHz录制。随机选择280个语音作为训练集，其余40个语音用作测试集。

### B Audio Feature Extraction

我们将音频信号重新采样到$16\text{kHz}$，并且仅使用单声道进行进一步处理。语音信号被转换到频域，并使用短时傅立叶变换处理成一系列帧。每个帧包含一个32毫秒的窗口，并且窗口重叠率为37.5％。因此，每秒有50帧。对于每个语音帧，我们提取对数功率谱，并通过去除均值并除以标准偏差来对值进行归一化。在句子水平上进行归一化处理，即，从一句的所有帧计算均值和标准差向量。我们将$±2$帧连接到中心帧作为上下文窗口。因此，每个时间步长的音频特征尺寸为$257\times5$。我们分别使用$X$和$Y$来表示嘈杂的和干净的语音特征。

### C Visual Feature Extraction

对于视觉流，我们将包含话语的每个视频以50 fps的固定帧速率转换为图像序列，以保持语音帧和图像帧的同步。接下来，我们使用Viola-Jones方法[62]检测到嘴巴，将裁剪后的嘴巴区域调整为16像素$\times$24像素，并保留其RGB通道。在每个通道中，我们在0到1的范围内重新缩放图像像素强度。我们减去平均值，然后将其除以标准差以进行归一化。对每个颜色的口腔图像进行该归一化。此外，我们将$±2$帧连接到中心帧，从而在每个时间步长生成尺寸为$16\times24\times3\times5$的视觉特征。我们用$Z$代表输入的视觉特征。

对于每句话，必要时使用截断法将音频频谱图的帧数和嘴巴图像的数量设为相同。

## 3 Audio-Visual Deep Convolution Neural Networks (AVDCNN)

所提出的AVDCNN模型的体系结构如图1所示。它由两个分别处理音频和视频流的独立网络组成，即音频网络和视觉网络。两个网络的输出融合后送入另一个被称为融合网络的网络。该图中的CNN，max_pooling层和全连接层缩写为$\text{Conv}_a1$,$\text{Conv}_2$,$\text{Conv}_v1$,$\cdots$,$\text{Pool}_a1$,$FC1$,$FC2$,$FC_a3$和$FC_v3$，其中下标$a$和$v$-分别表示音频和视频流。在以下部分中，我们描述AVDCNN模型的训练过程。

图1.提出的AVDCNN模型的体系结构。

### A Training the AVDCNN Model

为了训练AVDCNN模型，我们首先准备带噪-干净的语音对和嘴巴图像。如第二部分B和C部分所述，我们获得了噪声谱$(X)$和纯谱$(Y)$的对数幅度以及相应的视觉特征$(Z)$。对于每个时间步，我们获得音频网络的输出为:
$$
A_i=Conv_a2(Pool_a1(Conv_a1(X_i))),i=1,...,K \tag{1}
$$
其中，$K$是训练的样本个数，视频网络的输出为:
$$
V_i=Conv_a3(Conv_a2(Conv_a1(Z_i))),i=1,...,K \tag{2}
$$

接下来，我们将$A_i$和$V_i$展平，并将这两个特征连接为融合网络的输入，$F_i=[A'_𝑖,V'_𝑖]'$。前馈级联全连接网络的计算公式为：
$$
\hat{Y}_i=FC_a3(FC_a2(FC_a1(F_i))), i=1,\cdots,K \tag{3}
$$
$$
\hat{Z}_i=FC_v3(FC_v2(FC_v1(F_i))), i=1,\cdots,K \tag{4}
$$

AVDCNN模型的参数$\theta$随机在-1和1之间初始化，并通过使用反向传播优化以下目标函数进行训练:
$$
\min _{\theta}(\frac{1}{K} \sum_{i=1}^{K}\|\hat{Y}_{i}-Y_{i}\|_{2}^{2}+\mu\|\hat{Z}_{i}-Z_{i}\|_{2}^{2}) \tag{5}
$$
其中$\mu$是混合权重。

AVDCNN模型的CNN的步长为1×1，在FC1和FC2之后采用0.1的dropout以防止过拟合。bn应用于模型中的每个层。其他配置细节见表1。

### B Using the AVDCNN Model for Speech Enhancement

在测试阶段，将嘈杂语音信号的对数幅度和相应的视觉特征输入到训练后的AVDCNN模型中，以获得增强语音信号的对数幅度和相应的视觉特征作为输出。与频谱恢复方法类似，将嘈杂语音的相位作为增强语音的相位。然后，使用AVDCNN增强的幅度和相位信息来合成增强语音。我们仅将训练后的AVDCNN模型输出中的视觉特征视为辅助信息。这种特殊的设计使AVDCNN模型可以同时处理音频和视频信息。因此，训练过程是以多任务学习的方式执行的，事实证明，与在多个任务中的单任务学习[63，64]相比，该方法具有更好的性能。

Figure2. ADCNN模型的体系结构，与图1中的AVDCNN模型相同，其中视觉部分已断开。

Table1. AVDCNN模型的配置

### C Baseline Models

在这项工作中，我们将提出的AVDCNN模型与三个纯音频基准模型进行了比较。第一个是纯音频深度CNNs（ADCNN）模型。如图2所示，ADCNN模型断开了AVDCNN模型中所有与视觉相关的部分（请参见图1），并保留其余配置。第二个和第三个是两个常规的SE方法，即Karhunen-Loéve变换（KLT）[65]和对数最小均方误差（logMMSE）[66，67]。

另外，采用[51]中的用AVDNN表示的视听SE模型作为视听基线模型。AVDNN模型采用手工制作的视听特征，包括梅尔滤波器组和嘴唇轮廓中各点之间的相互距离变化。AVDCNN和AVDNN之间的另一个主要区别是，AVDNN模型基于DNN，并且不采用多任务学习方案，而AVDCNN通过在输出层考虑音频和视频信息来应用多任务学习。

## 4 Experiments and Results

### A Experiment Setup

在本节中，我们描述了这项研究中语音增强任务的实验装置。为了准备干净-带噪的语音对，我们遵循先前研究的概念[68]，其中考虑了干扰噪声和环境噪声的影响。对于训练集，我们使用了91种不同的噪声类型作为干扰噪声。这91种噪声是[69，70]中使用的104种噪声类型的子集。删除了与测试噪声类型相似的13种噪声类型。在五个行驶条件下的汽车发动机噪音被用来形成环境噪音，包括怠速发动机，升起窗口时每小时35英里，降窗时每小时35英里，降窗时每小时55英里，降窗时每小时55英里。汽车发动机噪声来自AVICAR数据集[71]。我们将这些汽车噪声串联起来，形成最终的环境噪声源进行训练。为了形成训练集，我们首先从320个干净的语音中随机选择了280个。干净的语音被人工混合了91种噪声类型，分别为10dB，6dB，2dB，-2dB和-6dB信号干扰噪声比（SIR），以及环境噪声分别为10dB，6dB，2dB，-2dB和-6dB的信噪比（SAR），导致总计$280\times91\times5\times5$句话。

接下来，为了形成测试集，我们采用了10种干扰噪声，包括婴儿的哭声，纯音乐，带歌词的音乐，警笛，一个背景通话器（1T），两个背景通话器（2T）和三个背景通话器（3T），因此1T，2T和3T背景说话者的声音有两种模式：空中录制和房间录制。在训练集中看不到这些噪音，即采用了噪音不匹配的情况。此外，之所以选择它们，是因为我们打算在测试场景中模拟一个令人沮丧的情况，例如在听后排座位的讲话者和汽车发动机发出噪音的同时驾驶收音机收听广播，因为视听信号处理技术具有此外，测试的环境噪声是从[75]中使用的数据集中获得的60 mph的汽车发动机噪声，这也与训练集中使用的噪声不同。因此，为了进行测试，共有40句干净的语音，与10dB，5dB，0dB和-5dB的SIR噪声类型以及5dB，0dB和-5dB SAR的环境噪声混合在一起，总共产生了$(40\times10\times3\times3)$句话。

我们使用随机梯度下降和RMSprop [76]作为学习优化器来训练神经网络模型，初始学习率为0.0001。我们选择模型的权重，在接下来的20个时期中，训练损失的改进小于0.1％。该实现基于Keras [77]库。

Figure3. 频谱图的比较：（a）干净的语音，（b）5dB SIR的3T（房间）噪声与-5dB SAR的环境噪声的嘈杂语音，以及通过（c）logMMSE增强的语音，（d）KLT，（e）AVDNN，（f）ADCNN和（g）AVDCNN

### B Comparison of Spectrograms

图3（a）-（g）分别显示了干净的语音，带有5dB SIR和5dBSAR的3T（房间）噪声与3T（房间）噪声混合的频谱图，以及分别由logMMSE，KLT，AVDNN，ADCNN和AVDCNN方法增强的语音的频谱图。显然，所有三种纯音频SE方法都无法有效地去除噪声成分。对于在发声开始和结束的静音部分仍然可以观察到噪声成分，这种现象特别明显。相反，在辅助视觉信息的帮助下，AVDCNN模型有效地抑制了嘴巴闭合部分的噪声成分。

但是，即使有附加的视觉信息，[51]中的AVDNN模型也无法获得与AVDCNN一样令人满意的结果。在[51]中，AVDNN的测试条件是噪声匹配的情况。然而，在这项研究中，条件更具挑战性，在这种情况下，AVDNN似乎无法像AVDCNN一样有效。如图3（e）和（g）所示，AVDNN的无效方面包括闭口时降噪不完全以及目标语音信号重建不佳。这些结果可能源于AVDNN的视觉功能不足，这表明CNN可以直接从图像中学习到的视觉功能要比AVDNN中使用的手工特征更强大。总结本小节，图3中的频谱图表明，所提出的AVDCNN模型比其他基准SE模型更强大，这在下一个小节中也得到了工具测量的支持。

### C Resultsof Instrumental Measures

在本小节中，我们将根据五个工具指标（PESQ，STOI，SDI，HASQI和HASPI）报告5个SE方法的结果。

PESQ度量（范围从0.5到4.5）指示增强语音的质量度量。STOI度量（范围从0到1）指示增强语音的清晰度。HASQI和HASPI措施（范围从0到1）分别针对正常听力和听力受损的人（通过设置特定模式）评估声音质量和感知。在这项研究中，HASQI和HASPI措施均采用正常听力模式。SDI度量计算干净语音和增强语音的失真度量。除了SDI以外，更大的值表示更好的性能。我们报告了在不同噪声类型以及SIR和SAR条件下，40句测试句子的平均评估得分。

我们首先打算研究不同噪声类型下的SE性能。图4-8分别显示了10种不同SIR噪声的平均PESQ，STOI，SDI，HASQI和HASPI分数，以及使用不同SE方法获得的增强语音，其中SAR固定为0dB。参见图4-8，我们首先注意到两种常规的SE方法（logMMSE和KLT）的性能表明它们不能有效地处理非稳态噪声。接下来，在比较两个基于CNN的模型时，AVDCNN在所有评估指标上均持续优于ADCNN，从而确认了视音频信息组合的有效性，从而获得了更好的SE性能。此外，AVDCNN展示了其作为一个视听模型的有效性通过在所有指标中表现优于AVDNN。为了进一步确认在图4-8中的每种测试条件下，AVDCNN模型相对于次优系统的优越性，我们进行了单向方差分析（ANOVA）和Tukey事后比较（TPHC）[78]。结果证实，这些分数的差异很大，在大多数情况下，p值均小于0.05，但STOI（带有音乐和警报音），SDI（带有婴儿哭声，音乐和警报音）和HASPI（带有音乐和警报音）除外。通过对实验结果的进一步分析，我们注意到，在10种测试噪声类型中，婴儿哭声的评估得分始终不及其他噪声类型的得分，这表明婴儿哭声的处理难度相对较大。同时，多背景通话者（2T，3T）场景似乎比单背景通话者（1T）更具挑战性。

接下来，我们比较了不同SE模型在不同SAR水平下提供的SE性能。图9-13显示了在特定SIR（超过10种不同的噪声类型）和SAR级别下，噪声和增强语音的PESQ，STOI，SDI，HASQI和HASPI的平均得分。在这些图中，$\times$，$\square$和$\circ$分别表示5dB，0dB，-5dB SAR。请注意，具有较高SAR的语音信号表示涉及较少的汽车发动机噪声分量。从图9-13可以看出，（1）较高SAR级别的仪器评估结果通常优于那些低SAR级别；（2）AVDCNN优于其他SE方法，这在较低SIR级别尤其明显。该结果表明，视觉信息为在更具挑战性的条件下协助AVDCNN进行SE提供了重要线索。

Figure4. 考虑了不同的增强方法和SAR为0 dB时的噪音类型，平均10种不同的带噪和相应的增强语音版本的PESQ得分。

Figure5. 考虑了不同的增强方法和SAR为0 dB时的噪音类型，平均10种不同带噪和相应语音增强版本的STOI得分

Figure6. 考虑了不同的增强方法和SAR为0 dB时的噪音类型，平均10种不同带噪和相应语音增强版本的SDI得分

Figure7. 考虑了不同的增强方法和SAR为0 dB时的噪音类型，平均10种不同带噪和相应语音增强版本的HASQI得分

Figure8. 在10 dB SAR时考虑了不同的增强方法和变化的噪声类型后，平均10种不同的带噪的和相应增强语音版本的HASPI得分

Figure9. 在考虑每种SIR和SAR的不同增强方法后，超过10种不同版本的噪声和相应的增强语音的平均PESQ得分

Figure10. 考虑到每个SIR和SAR的不同增强方法，超过10种不同版本的带噪声和相应的增强语音的平均STOI得分

Figure11. 考虑到每个SIR和SAR的不同增强方法，超过10种不同版本的带噪声和相应的增强语音的平均SDI得分

Figure12. 考虑到每个SIR和SAR的不同增强方法，超过10种不同版本的带噪声和相应的增强语音的平均HASQI得分

Figure11. 考虑到每个SIR和SAR的不同增强方法，超过10种不同版本的带噪声和相应的增强语音的平均HASPI得分

### D Multi-style Training Strategy

先前的研究[79]表明，多模态网络的某种模态的输入可能会超过其他输入类型。在我们的初步实验中，我们观察到了相似的特性。为了缓解这个问题，我们采用了多风格训练策略[80]，该策略在训练阶段的每45个epoch中随机选择以下输入类型：视听，仅视频和仅音频。在仅视频输入时，将音频输入设为零，提供视觉输出，而音频输出则根据两种不同的模式进行了设置：模式1将音频目标设置为零，模式2使用纯净的音频信号作为目标。同样，当使用仅音频数据并将视觉输入设置为零时，模式1将视觉目标设置为零，模式2将原始视觉数据用于视觉目标。应注意，模式1和模式2是通过多风格训练策略进行训练的，不同之处在于训练过程中输出中指定的信息。来自模式1和模式2的训练过程的均方误差（MSE）分别在图14和图15中列出。在图14和15中，我们使用长条标记了三种类型的输入的时间段，即视听，纯视觉和纯音频。

参照图14和15所示的结果，我们可以看到对包含视觉信息的一些支持。从这两个图中用红色实线表示的窗口中，我们注意到当我们使用纯音频数据进行训练时，音频损失相对较大。视觉功能一旦完成，MSE就会下降到较低水平，这表明音频和视觉流之间存在很强的相关性。

Figure14. 使用Model-1的多样式学习模型的训练数据的学习曲线模型I表示在训练中仅选择了音频/视频输入时，将视频/音频目标设置为零。红色框表示由于包含了附加视觉信息，因此可以实现较小的音频损失。

Figure15. 使用Model-2的多样式学习模型的训练数据的学习曲线表示在训练中仅选择音频/视频输入时保留视觉/音频目标。红框表示，由于包含附加的视觉信息，因此可以实现较小的音频损失。

### E Mixing Weight

在上述实验中，将等式（5）中的混合权重表示固定为1。即，在训练AVDCNN的模型参数时，将误差视为同等有害。在本小节中，我们将探讨$\mu$与SE性能之间的相关性。图16显示了在AVDCNN模型的训练过程中，在不同混合权重下训练数据中的视听损失。观察到，我们越重视视觉信息，即混合权重值$\mu$越大，我们获得了更好的视觉损失和更差的音频损失。考虑到音频损耗主导了增强效果，我们倾向于选择一个较小的$\mu$。

### F Multimodal Inputs with Mismatched Visual Features

在本小节中，我们显示正确地匹配输入音频特征与其视觉对应特征的重要性。我们在语音中选择八个嘴形作为固定的视觉单元，然后针对每个“snapshot”将其固定为整个视觉特征。发声。从图17的频谱图中可以看出，具有正确嘴唇特征的AVDCNN增强语音比具有错误嘴唇特征序列的其他AVDCNN增强语音信号保留了更详细的结构。具有正确视觉特征的40种测试句子的平均PESQ为2.54，八个假嘴唇形状序列的增强语音信号的平均得分范围为1.17至2.07。这些结果表明，嘴唇形状的提取显着影响了AVDCNN的性能。

Figure16 AVDCNN模型训练过程中不同混合权重下训练数据中的视听损失

Figure17（a）在0dB SIR时1T（空中）噪声的嘈杂语音，（b）干净语音，（c）具有正确嘴唇特征的AVDCNN增强语音，（d）-（k）（左）选定的嘴唇形状，（右）具有不正确的嘴唇特征的AVDCNN增强语音，这些特征是选定嘴唇形状的序列。

### G Reconstructed Mouth Images

在提出的AVDCNN系统中，我们使用视觉输入作为语音信号的辅助线索，并在输出中添加视觉信息，作为模型训练期间约束的一部分。因此，所提出的系统实际上是具有多任务学习的视听编码器-解码器系统。除增强语音帧外，我们还在测试阶段的输出端收到了相应的口部图像。研究使用视听编码器-解码器系统获得的图像很有趣。图18显示了一些可视化的样本。目前，与目标增强型语音信号相比，我们仅将这些图像视为视听系统的“副产品”。但是，将来，当相应的视觉提示严重损坏或未提供时，探索模型学习的嘴唇形状将很有趣。

Figure18. 可视化标准化嘴图像（a）建议的AVDCNN模型的视觉输入和（b）视觉输出。（c）（a）与（b）之差，振幅放大十倍。

### H Subjective Listening Tests

除了进行仪器评估外，我们还通过三种不同的方法，即logMMSE，ADCNN和AVDCNN，的增强语音进行了主观听力测试。我们采用[81]的听力测试程序，使用五点量表来评估背景噪声干扰度（BAK）和整体效果（OVRL）。分数越高越有利。通过上述三个模型，每个受试者从-5dB SIR和-5dB SAR下的所有10种测试噪声中聆听了10个句子，从而产生总计（3×10×10）个句子。共有20名母语为普通话的受试者参加了测试。受试者年龄在23岁至40岁之间，平均年龄为26岁。表II中列出了受试者的平均得分。结果表明，所提出的AVDCNN模型在主观听觉测验中获得了三个模型中的最佳成绩。

Table3. 不同模型的主观听觉测试的结果

### I Early Fusion Scheme for the AVDCNN Model

我们还尝试了一种早期融合方案，通过在输入进入卷积层之前将输入的视觉和视觉特征相结合。早期融合模型，以AVDCNN-EF表示。用合并的CNN代替了图1中的音频网络和可视网络，其输入包括通过合并音频特征生成的融合视听特征，分离的RGB视觉特征通道和零填充，最终形状为$257\times29\times1$（音频：$257\times5\times1$，RGB：$(80+80+80)\times24\times1$，零填充：$17\times24\times1$）。AVDCNN和AVDCNN-EF的参数数目是相同的。表3中对AVDCNN和AVDCNN-EF的增强结果的仪器指标进行了比较。分数代表在0dB SAR下在不同SIR下10种不同噪声下增强语音的平均分数。显然，AVDCNN始终优于AVDCNN-EF，这表明所建议的融合方案首先对音频和视觉信息进行单独处理，然后再对其进行融合，比早期的融合方案要好，后者在开始时就将异构数据进行了合并。

Table3. 在0 dB SAR的不同噪声下，与没有融合的AVDCNN模型相比，超过10种不同噪声的增强了语音的平均度量

### V Discussion

从以前的实验中，我们可以清楚地看到视觉信息如何影响增强结果。例如，图3（g）显示，当嘴巴闭合时，来自非目标说话者的噪声和语音信号得到了有效抑制。这表明视觉信息在语音活动检测（VAD）中起着有益的作用。实际上，有研究人员朝着这个特定方向开展工作[82，83]。这也有助于我们选择车内环境作为测试场景，并研究视听SE的效果。如果有一个针对驾驶员口部区域的摄像头，则唇形可以强烈提示是否要使用背景通话者或噪音来激活语音命令系统，此外还可以增强语音。唇形可以为VAD提供有用的提示。但是，这似乎还不是很可靠。如图19所示，在使用AVDCNN模型进行的增强语音测试的一些结果中，我们观察到由于当时的嘴巴张开，在非语音段中噪声成分未完全去除。我们认为，可以通过结合纯音频VAD技术来进一步改善这一缺点。

我们还根据实际测试数据初步评估了AVDCNN模型，即在真实的嘈杂环境中记录了嘈杂的语音，而不是人为地将噪音添加到干净的语音中。图20（a）说明了用于记录训练和测试数据的受控环境。图20（b）说明了通过智能手机（ASUS ZenFone 2 ZE551ML）在夜市中记录的真实数据的记录条件。噪声和AVDCNN增强的语音信号的频谱图分别显示在图20（c）和（d）中。红色框表示目标语音与背景谈话声音混合在一起的片段。我们观察到唇形有助于识别目标语音段，而目标语音的重建效果不如受控环境下的增强结果。这可能是由于不同的光线覆盖范围，较低的SIR或背景噪声的特性所致。在实际测试条件下仍有改进视听SE的空间。

Figure19:（a）干净语音和（b）AVDCNN增强语音的频谱图。 （b）中的红色框表明，如果嘴巴未闭合，则在非语音段中，噪声不会完全降低。

Figure20. 在真实条件下进行测试。（a）用于记录训练和测试数据的受控环境（研讨会室）。 （b）真实测试数据的记录环境（夜市）。 （c）带有噪声的嘈杂语音的频谱图，（d）来自AVDCNN模型的增强语音的频谱图。（c）中的红色框表示目标语音与噪声混合的部分。

## 5 Conclusion

在本文中，我们提出了一种新颖的基于CNN的具有多任务学习的语音增强视音频编解码器系统，称为AVDCNN。该模型利用单个网络来处理具有不同模态的输入数据，然后使用融合网络来学习联合多模态特征。我们以端到端的方式训练了模型。使用所提出的体系结构获得的实验结果表明，就5个仪器评估指标而言，其在SE任务上的性能优于3种纯音频基线模型，证实了将视觉信息与音频信息集成到SE过程中的有效性。还通过将其与其他视听SEmodel进行比较来证明该模型的有效性。总体而言，本文的贡献有五方面。首先，我们在提议的端到端视听SE模型中将CNN用于音频和视频流，从而在许多基准模型上获得了改进。其次，我们量化了通过多模式和多任务培训策略为SE集成可视信息的优势。第三，我们证明了后期融合处理音频和视频流要比早期融合更好。第四，实验结果显示了语音和嘴唇形状之间的高度相关性，并显示了在视听SE中使用正确的嘴唇形状的重要性。最后，我们证明了嘴唇形状作为VAD的辅助特征是有效的，并指出了使用视听SE模型时的潜在问题。将来，我们将尝试通过使用整个人脸而不是仅嘴部区域作为视觉输入来改进提议的体系结构，以便利用训练有素的人脸识别网络来改善视觉描述符网络。此外，我们计划通过考虑其他基于CNN的技术模型，例如完全卷积网络[84-86]和U-Net [87]，来修改模型中的现有CNN。一种更复杂的同步方法可能会提高性能，值得研究。最后，为了提高模型在实际应用场景中的实用性，我们将考虑收集包括更复杂和真实条件的训练数据。
