# Looking to Listen at the Cocktail Party: A Speaker-Independent Audio-Visual Model for Speech Separation

这项工作由google的研究者们在

Figure 1. 我们提出了一种用于隔离和增强视频中所需说话者语音的模型。 （a）输入是一个视频(帧+音轨)，其中有一个或多个人在讲话，其中感兴趣的语音受到其他说话人和/或背景噪声的干扰。 （b）提取音频和视觉特征并将其输入到联合视听语音分离模型中。输出是将输入音轨分解为干净的音轨，每个视频中检测到的人都有一个清晰的语音轨道（c）这样，我们就可以创作视频，在其中增强特定人的语音而抑制其他所有声音。我们使用新数据集AVSpeech中的数千小时视频片段对模型进行了训练。 “Stand-Up”视频（a）由Team Coco提供。

我们提出了一种联合视听模型，用于将声音信号从其他扬声器和背景噪声等混合声音中隔离出来，仅使用音频作为输入来解决此任务非常具有挑战性，并且无法提供分离的语音信号与扬声器中的扬声器的关联视频。在本文中，我们提出了一个基于深度网络的模型，该模型结合了视觉和听觉信号来解决此任务。视觉功能用于将音频“聚焦”到场景中所需的扬声器上，并改善语音分离质量。为了训练我们的联合视听模型，我们引入了AVSpeech，这是一个新数据集，其中包含来自网络的数千小时的视频片段。我们展示了我们的方法适用于经典语音分离任务以及涉及激烈访谈，嘈杂酒吧和尖叫儿童的真实场景的适用性，仅要求用户在视频中指定他们想要隔离的人的脸部即可。在混合语音的情况下，我们的方法显示出优于最新的纯音频语音分离的优势。此外，我们的模型与说话者无关（训练过一次，适用于任何说话者）