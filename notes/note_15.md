# Seeing Through Noise: Visually Driven Speaker Separation And Enhancement

这是一篇来自耶路撒冷希伯来大学的研究者的有关音视频语音增强和分离的技术类paper，发表时间2018年2月，被ICASSP2018接收。

## Abstract

当在嘈杂的环境中拍摄视频时，如何在过滤其他声音或背景噪音的同时隔离特定人的声音具有挑战性。我们提出了视听方法，以隔离单个说话人的声音并消除无关的声音。首先，通过将无声视频帧通过基于视频到语音的神经网络模型，将视频中捕获的面部动作用于估计说话者的声音。然后，将语音预测作为滤波器应用于有噪声的输入音频。这种方法避免了在学习过程中使用声音的混合，因为这种可能的混合的数量巨大，并且不可避免地会偏向训练后的模型。我们在两个视听数据集GRID和TCD-TIMIT上评估了我们的方法，并证明了我们的方法相对于众所周知的纯音频方法和原始视频到语音的预测，能够达到SDR和PESQ显着改善。

## 1. Intrduction

单通道说话人分离和语音增强已经得到了广泛的研究[1、2]。最近对神经网络进行了训练，可以将音频混合分离到它们的源中[3]。这些模型能够学习独特的语音特征，如谱带，音高和chrip [4]。纯音频方法的主要困难是它们在分离相似的人类声音（例如相同性别的混合物）方面表现不佳。

我们首先描述面部在视频中可见的两个说话者的混合语音的分离。我们继续将单个可见说话人的语音与背景声音隔离开来。这项工作建立在机器语音读取的最新进展的基础上，通过面部和嘴巴的可见运动生成语音[5，6，7]。

与其他利用在语音和噪声或两种声音的混合中训练模型的方法不同，我们的方法是依赖于说话者且噪声不变的。这样一来，即使在同一个人的两个声音重叠的情况下，我们也可以使用少得多的数据来训练模型，并且仍然可以获得良好的结果。

### 1.1. Related Work

**纯音频语音增强和分离** 用于单通道或单声道语音增强和分离的先前方法大多使用纯音频输入。通用频谱mask方法会生成mask矩阵，其中包含每个说话者占主导地位的时频（TF）分量[8、9]。Huang [10]这是第一个使用基于深度学习的方法进行与说话者相关的语音分离的方法。

Isiket等人[4]通过深度聚类来解决单通道多扬声器分离问题，其中以区别训练的语音嵌入作为聚类和分离语音的基础。 Kolbaeket等人[11]介绍一种简单的方法，其中他们使用排列不变的损失函数，这有助于基础神经网络在不同的说话者之间进行区分。

**视听语音处理** 最近在视听语音处理中的研究广泛使用了神经网络。 Ngiamet等人的工作[12]是这方面的开创性工作。具有视觉输入的神经网络已用于唇读[13]，声音预测[14]和学习无监督的声音表示[15]。

关于视听语音的增强和分离的工作也已经完成[16，17]。 Kahn和Milner [18，19]使用手工制作的视觉特征来导出用于扬声器分离的二进制和软mask。 Houet等人[20]提出了基于CNN的模型来增强嘈杂的语音。他们的网络生成表示语音增强的频谱图。

## 2. VISUALLY-DERIVED SPEECH GENERATION

存在几种从说话人的无声视频帧中产生可理解语音的方法[5、6、7]。在这项工作中，我们依靠vid2speech [6]，在第2节中进行了简要介绍。 2.1。应当注意的是，这些方法取决于说话者，这意味着必须为每个说话者训练一个单独的专用模型。


