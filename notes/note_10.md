# CochleaNet: A Robust Language-independentAudio-Visual Model for Speech Enhancement

## Abstract

带噪的环境会导致听力损失，因为助听器会使信号更容易听见，但并不能始终恢复清晰度。在带噪的环境中，人们通常会利用语音的视听(AV) 特性来有选择地抑制背景噪音和专注于目标说话人。在本文中，我们提出了一种因果的，语言，噪声和说话人都无关的AV深层神经网络(DNN) 架构来增强语音(SE) 。该模型利用了带噪的声音提示和对噪声鲁棒的视觉提示，将注意力集中在所需的说话人上，并提高了语音清晰度。为了评估提出的SE框架，首先在包括餐厅和饭店在内的实际嘈杂环境中记录了称为ASPIRE的AV立体声的语料库。我们在客观测量和主观听觉测试方面展示了该方法相对于最新的SE方法以及最近基于DNN的SE模型的优越性能。此外，我们的工作挑战了一种普遍的观念，即缺乏多语言的大词汇量AV语料库和各种各样的噪音是构建健壮的语言，说话人和噪音无关的SE系统的主要瓶颈。我们显示，在Grid语料库(具有33个说话人和一个小的英语词汇表) 和ChiME 3 Noises(仅由公共汽车，行人，食堂和街道噪音组成) 的合成混合物上训练的模型不仅在大型词汇集上而且在完全无关的语言(例如普通话) ，多种说话人和噪声上均具有良好的泛化性能。

## 1 Introduction

人类听觉皮层具有通过选择性抑制环境噪声而专注于目标语音的卓越能力。众所周知，选择性抑制不必要的背景噪声会利用对噪声鲁棒的视觉提示来增强人的解决语音歧义的能力[1]。另外，研究表明视觉提示在提高语音清晰度[2]以及在带噪环境中进行语音检测[3，4]的重要性。在这项研究中，我们通过计算实现了这种选择性语音增强能力。

近年来，语音增强(SE) 的降噪能力吸引了人们的广泛关注，该功能有助于在带噪的社交场合中让受损的听力听的更好，并为语音处理系统(例如语音识别和语音活动检测器系统) 在带噪的环境中打开了大门[5，6]。SE方法可以分类为基于统计分析的降噪模型(例如频谱减法(SS) ，线性最小均方误差(LMMSE) 和维纳滤波) 和计算听觉场景分析(CASA) [7]。已经观察到，由于引入了诸如音乐噪声之类的失真，统计方法在某些情况下无法实现语音清晰度的提高。相反，CASA已显示出在平稳和非平稳噪声方面更为有效[8]。

在CASA中，通过对带噪语音的T-F表示使用时频(T-F) 频谱mask将语音与干扰的背景噪声分离开来。T-F频谱mask用于增强语音的主要区域并抑制噪声占主导的区域。如果局部信噪比(SNR) 低于局部标准(LC) ，则理想二进制 mask(IBM) 将零分配给T-F单元，否则将1分配给T-F单元。IBM的定义如下：
$$
IBM(t,f)=\{\begin{array}{ll}
0 & \text { if } SNR(t,f) \leq \mathrm{LC} \\
1 & \text {otherwise}
\end{array} \tag{1}
$$

IBM已证明可以改善听力受损和听力正常的听众的语音质量和清晰度[9，10，11]。在现实情况下，无法使用公式1计算IBM，因为目标语音和干扰背景噪声无法高精度估算。但是，IBM估计可以建模为数据驱动的优化问题，该问题联合利用带噪的语音和可视人脸图像进行频谱掩模估计。

在文献中，已经进行了广泛的研究以开发纯音频(A-only) 和视听(AV) SE方法。研究人员提出了几种SE模型，例如基于深度神经网络(DNN) 的频谱mask估计模型[12、13]，基于DNN的干净频谱图估计模型[14,15]，基于Wiener滤波的混合模型[16、17、18]和时间域SE模型[19，20，21]。但是，在日常社交环境(如自助餐厅和餐馆) 中观察到的低SNR(<-3 dB) 的场景下，传统的纯音频的助听器无法提高语音清晰度，只有有限的工作在开发鲁棒的，因果的，语言，说话人和噪声无关的AV SE模型。

另外，前面提到的AV SE研究都没有对真实的噪声混合物进行听觉测试，这些噪声混合物通常是由语音信号与多个竞争背景噪声源进行带混响地混合而成的[22]。最后，研究表明，基于预训练DNN的SE模型不能很好地推广到新语言上[23]。可以在大型AV语料库上对该模型进行微调，该语料库由多种语言组成，例如AVSPEECH [12](由1500小时记录组成) ，只要具有足够的模型容量，就可以潜在地实现与语言无关的性能。但是，像AVSPEECH这样的语料库培训需要大量的图形处理单元(GPU) 或张量处理单元(TPU) ，而这些在学术研究环境中通常是不可用的。

在本文中，我们提出了一种因果的，语言，噪声和说话人无关的AV模型，以通过有选择地抑制背景噪声来专注于目标说话人。更具体地说，我们设计并训练了一种称为CochleaNet的交叉模态的DNN架构，该架构吸收了带噪的语音混合和说话人嘴唇的裁剪图像作为输入，并输出了一个T-Fmask，以选择性地抑制和增强每个T-F单元。此外，该模型根据上下文利用可用的AV提示来估计与SNR独立的频谱mask。

提出的AV SE模型是使用ASPIRE进行评估的，ASPIRE是首个在真实的带噪环境下(例如咖啡馆和餐厅) 录制的高品质的AV立体声语料库。要注意的是，大多数前述的AV SE方法使用纯净语音和噪声的合成混合物进行模型评估。然而，合成的混合物不能反映真实的带噪混合物，因为语音常常与多种竞争性噪声背景源带混响地混合。因此，语音和机器学习社区可以将ASPIRE语料库用作基准资源，以支持对AV SE技术的可靠评估。

与最新的A-only SE方法以及最新的基于DNN的SE模型相比，我们展示了提出方法的卓越的语音质量和清晰度。另外，我们显示了一个在Grid语料库[24](只有33个说话人和一个小英语vocab-ulary) 和ChiME 3 [22]噪声(由公共汽车，行人，咖啡馆和街道噪声组成) 的合成混合物上训练的模型对真正带噪的ASPIRE语料库，大型词汇语料库(例如TCD-TIMIT [25]) ，其他语言(例如普通话[15]) 以及各种各样的说话人和噪音[26、27]具有很好的泛化性能。我们提出的AV SE模型的概述如图1所示。

总而言之，我们的论文提出了六个主要贡献：

1. 提出了一个因果的，语言，噪声和说话人无关的AV DNN驱动的SE模型。该模型在上下文中独立于SNR利用音频和视频提示来估计用于选择性抑制和增强每个T-F单元的频谱mask。
2. 收集首个此类AV语料库，该语料库由在诸如餐厅和饭店等真实带噪环境中录制的高质量立体声语音组成，以评估所提出模型在具有挑战性的真实带噪环境中的性能。在文献中，通常使用干净语音和噪声的合成混合物来评估AV SE方法。然而，合成的混合物并不能描述真正的带噪混合物，因为在真实混合物中，语音与多种竞争性背景噪声带混响地混合。
3. 就我们所知，本文第一个提出了一种说话人，噪声和语言无关的模型，即使在一个小的英语词汇网格语料库上进行训练之后，该模型也可以推广使用不同的语言。在文献中，已经表明，使用单一语言训练的预训练SE模型在新语言上效果不佳[23]。
4. 通过使用真实的ASPIRE语料库，最新的A-only SE方法(包括频谱减法，线性最小均方误差) 以及最近基于DNN的SE模型(包括SEGAN) ，对我们提出的方法进行了广泛的评估 ) 同时使用客观指标(PESQ，SI-SDR和ESTOI) 和主观MUSHRA(带有隐藏参考和锚点的多刺激测试) 听力测试。
5. 我们还根据客观指标研究了由于闭塞而导致的视觉提示暂时或永久缺席的情况下经过训练的AV模型的行为。
6. 最后，我们批判性地分析和比较纯音频的CochleaNet模型与AV版本的性能，以凭经验确定在AV模型的性能中视觉线索的角色。具体来说，研究静音语音区域中仅A和AV模型的行为，以及我们进行听力测试以评估模型在不同音素上的性能。我们假设该模型在视觉上可区分的音素上比在视觉上无法区分的音素表现更好。

本文的其余部分安排如下：第2节简要回顾了相关工作，第3节介绍了ASPIRE语料库的设置和所涉及的后处理。第4节介绍了CochleaNet，一种用于SE的AV mask估计模型。第5节讨论实验设置和结果。第6节总结了这项工作并提出了未来的研究方向。

## 2 Related work

本节简要回顾了A-only和AVSE领域的相关工作。

### 2.1 Audio-Visual Speech Enhancement

Ephrat等[12]提出了一种独立于说话人的AV DNN，用于复数比率 mask估计，以将语音与重叠语音和背景噪声分开。该模型在AVSPEECH上进行了训练，AVSPEECH是一种新的大型AV语料库，包含1500小时的录制时间，并具有多种语言，人物和面部姿势。上述研究的主要局限性在于，模型是在固定的SNR上进行训练和评估的。同样，Gogate等[13]提出了一种独立于说话人的AV DNN，用于IBM估计，以将语音与背景噪声区分开。然而，该模型是使用有限的词汇表语料库训练和评估的[24]，这对实现卓越的性能有帮助。此外，Hou等人[15]提出了一种基于说话人有关的SE模型，在单个说话人上进行训练和评估，该模型使用多模态深度卷积网络从嘈杂的频谱图中预测增强的频谱图。另一方面，Gabbay等[14]训练了卷积编码器-解码器体系结构，以从嘈杂的语音频谱图和裁剪的嘴巴区域估计增强语音的频谱图。但是，当视觉效果被遮挡时，该模型将无法工作。Adeel等[17，18]通过整合增强的视觉驱动的维纳滤波器(EVWF) 和基于DNN的唇读回归模型，提出了仅视觉和AV SE模型。初步评估证明了在各种噪声环境中处理频谱时变的有效性。Owens等[28]提出了一个自监督训练的网络，以分类音频和视频流是否在时间上对齐。该模型然后用于特征提取，以控制开/关屏幕说话人源分离模型。Afouras等[29]训练了DNN来预测降噪语音频谱图的幅度和相位。最后，Zhao等人[30]提出了一个模型，用于将多个对象的声音与视频分开(例如乐器) 。

### 2.2 Audio-only Speech Enhancement

Hershey等[31]提出了深度聚类，利用区分性训练的语音embedding来聚类和分离不同的源。对于时域SE，Rethage等[19]提出了一种基于非因果Wavenet的SE模型，其对原始音频进行处理以解决基于频谱mask的模型中的无效短时傅立叶变换(STFT) 问题[32]。类似地，Pandey等[20]和Lou等[21]提出了一种全卷积时域SE模型，该模型解决了频域分离的缺点，包括相位和幅度的去耦，以及计算STFT的高延迟。

仅音频的SE和分离的基本问题是标签排列问题[31]，即没有简单的方法将音频源的混合与相应的说话人或乐器关联[33]。此外，大多数上述仅音频和AV SE方法的主要局限性在于，所开发的模型要么在高SNR(SNR> 0 dB) 上评估，要么在固定SNR上评估。另外，上述AV方法都没有在记录真实噪声的AV语音语料库的设置中进行评估。

## 3. ASPIRE Corpus

在文献中，已经进行了广泛的研究来开发纯音频噪声混合物，该混合物通常由语音信号与多种竞争性噪声背景源进行带混响地混合而成[22]。但是，据我们所知，没有可用的真实嘈杂环境下录制的AV语料库。在本节中，我们介绍ASPIRE，这是同类产品中的第一款，在真实的嘈杂环境(例如自助餐厅和饭店) 中录制的AV语音语料库，以支持对AV SE技术的可靠评估。

### 3.1. Sentence design

Table1：Grid语料库的句子结构 例如：place blue in A 9 soon

ASPIRE语料库与表1所示的AV Grid语料库采用相同的句子格式。六个单词的句子由命令，颜色，介词，字母，数字和副词组成。字母“ w”被排除在外，因为它是唯一的多音节字母。

每个说话人都产生了颜色，字母和数字的所有组合，从而在实际的嘈杂环境和隔声隔间中使每个讲话者发出1000句话。因此，每个说话人记录了2000句话。

### 3.2. Speaker population

三名说话人(一名男性和两名女性) 为语料库做出了贡献。说话人的年龄介于23至55岁之间。所有说话人的大部分时间都生活在英国，并且也包含一系列混合的英语口音。所有参与者都获得了他们的贡献。语料库总共包含6000句话(在实际的嘈杂环境中记录了3000句，在隔声隔间中记录了3000句) 。

### 3.3. Collection

Figure2：ASPIRE录音计划设置，显示了听众，说话人，录音机，录像机，句子提示器和双耳/领口麦克风的位置

ASPIRE语料库会在繁忙的午餐时间(11.30至1.30) 在实际带噪的环境中，尤其是大学食堂和餐厅以及隔音隔声室中记录。录制设置如图2所示。

Apple iPad mini 2置于视线高度，以避免噪声和视频设备的干扰，用于以每秒30帧(fps) 和1080p分辨率录制视频(iPad与说话人之间的距离为90厘米) 。一个领口麦克风也连接到iPad。还使用Zoom H4n pro记录器以44100 Hz的采样率和双耳麦克风记录来自说话人的高质量双耳音频。戴着双耳麦克风的收听者的距离约为140厘米。

听众和说话人坐在固定椅子上彼此相对。最初使用很少的句子对说话人进行的训练，而且还详细说明了研究的目的。录音期间会定期给说话人休息，以免造成疲劳，每句话都必须正确阅读，不得间断。将第3.1节中详细介绍的句子以随机顺序用笔记本电脑呈现给的说话人，并且如果句子录制中断或句子发声不正确，则允许说话人重复该句子。另外，如果听众发现有任何错误，说话人会重复说话。总计，每个说话人的句子量为2000(在实际带噪环境下为1000，在展位中为1000) ，分别在录音棚和实际噪音环境中分别重复记录了2％和4％的句子。

### 3.4. Postprocessing

**音频后处理** 在整个会话过程中不断收集音频和视频数据。音频和视频数据之间的漂移是通过使拍手同步来计算的。使用Gentle(建立在Kaldi上的强大的强制对准器) ，从领口麦克风记录的语音以及所呈现的转录识别发声的开始和结束时间。最后，手动检查所有分段的话语以纠正任何其他对齐错误。

**视频后期处理** 在繁忙的饭店和自助餐厅中录制的原始视频由说话者本身以外的一些清晰可辨的人组成。因此，为了确保隐私，我们使用分割模型来估计第一帧的说话者区域，并使用估计的分割mask对非说话者区域进行完整的像素化。这是可能的，因为说话人在整个讲话过程中都坐在单个位置。图3显示了来自ASPIRE语料库的一些示例视频帧。

## 4. CochleaNet

### 4.1 Data Representation

**输入特征** 我们的模型同时吸收了音频和视频作为输入。对于batch训练，考虑3秒的视频剪辑。从视频中提取裁剪的$80\times40$嘴唇区域，并将其用作视频输入(假设以25 fps的速度记录3秒剪辑，则制作了75个裁剪的嘴唇图像) 。对于音频输入，我们计算音频片段的STFT并使用幅度谱图。训练后的模型可以应用于推理期间的流数据以及任意长度的数据。

**输出** 我们网络的输出是一个IBM乘法谱图mask，它描述了干净音频和背景噪声之间的T-F关系。在文献中，已经显示出乘法mask比直接预测时域波形和干净的频谱图幅度更好[34，35]。

### 4.2. Network Architecture

图4：CochleaNet DNN体系结构概述：视听语音增强

本节描述了提出的AV SE模型的网络体系结构。图4概述了网络中存在的多流模块。后续小节将详细介绍每个模块。

#### 4.2.1. Audio Feature Extraction

音频特征提取包括如表2所示的膨胀卷积层。每个层后均跟随ReLU激活以实现非线性。

#### 4.2.2. Visual Feature Extraction

视觉特征提取包括扩张的卷积，最大池化和长短期记忆(LSTM) 层，如表3所示。每个卷积层后都有ReLU激活以实现非线性。

#### 4.2.3. Multimodal Fusion

视觉特征的采样率为25 fps，而音频特征的采样率为75矢量/秒(VPS) 。对视觉特征进行上采样，以匹配每秒的音频矢量速率，并补偿采样速率的差异。使用每个元素在时间维度上的简单重复3次即可完成此操作。上采样后，将音频和视频特征连接起来并馈送到由622个单元组成的LSTM层。然后将LSTM输出馈送到具有622个神经元和ReLUactivation的两个完全连接的层。整个连接层的权重在时间维度上共享。最后，将提取的特征馈入具有622个神经元和Sigmoid激活的全连接层。估计值和实际IBM之间的二进制交叉熵被用作损失函数。要注意的是，没有阈值被应用于预测mask，并且Sigmoid输出被认为是估计mask。

#### 4.3 Speech Resynthesis

当输入有噪声的频谱图和裁剪图像时，该模型将估算T-F IBM。估计的乘法频谱mask应用于噪声幅度频。然后，将mask后的幅度谱与带噪相位合并，以使用ISTFT获得增强的语音。图1概述了语音重新合成。

## 5. Experiments and Results

我们在实际带噪的环境中以及一系列合成的AV语料库中，使用其他最先进的A-only和AV SE定性和定量地评估了我们提出的方法。

### 5.1. Synthetic AV Corpora

本节介绍用于训练和测试CochleaNet的合成AV语料库。

#### 5.1.1. Grid + ChiMe 3

在我们的实验中，基准Grid语料库[24]用于所提出框架的训练和评估。所有33个具有1000句话的说话人都被考虑在内。句子的格式在表1中进行了描述。Grid语料库与来自第三次CHiME挑战(CHiME 3) [22]的非平稳噪声随机混合，该噪声由公共汽车，自助餐厅，街道和行人噪声组成，SNR范围为[-12，9 ] dB，步长为3 dB。要注意的是，训练后的模型是独立于SNR的，即，将所有SNR的话语组合起来进行训练和评估。为了进行训练，使用了来自21个说话人的21000句话。该模型分别在4个和8个说话人的4000和8000句话下进行了验证和测试。

#### 5.1.2. TCD-TIMIT + MUSAN

对于大词汇量泛化分析，我们使用基准TCD-TIMIT [25]语料库。具体来说，来自56个说话人的5488句话与来自MUSAN噪声的随机选择的非语音噪声混合在一起[27]。MUSAN噪声包括技术噪声(例如拨号音，传真机噪声等) 以及周围的声音(例如雷声，风声，脚步声，动物噪声等) 。要注意的是，所有5488语音都用作测试集，以评估大词汇量，说话者和噪声无关设置下的模型性能。

#### 5.1.3. Hou et al. + NOISEX-92

对于不依赖语言的泛化测试，基于台湾普通话噪声测试(MHINT) 的普通话数据集[15]，具有320句话，并混合了从NOISEX-92 [26]中随机选择的噪音，包括babble，工厂无线电频道和各种军事噪音包括战斗机，机舱，作战室，坦克和机枪。

### 5.2. Data Preprocessing

#### 5.2.1. Audio Preprocessing

音频信号以16 kHz重新采样，并使用单声道进行处理。重新采样的音频信号被分割为$N$个78毫秒(ms) 帧和17％的增量速率，以产生75 fps。应用汉宁窗和STFT来产生622点幅度谱图。

#### 5.2.2. Video Preprocessing

Grid和TCD-TIMIT语料库以$25\text{ fps}$录制。但是，以ffm-peg [36]将以$30\text{ fps}$记录的普通话数据集[15]降采样为$25\text{ fps}$。dlib人脸检测器[37]用于在视频剪辑的每一帧中定位人脸(假设以$25\text{ fps}$录制3秒剪辑，则制作了$75$张人脸裁剪图像) 。使用针对提取嘴唇界标而优化的缩小化dlib [37]模型，从$25\text{ fps}$面部视频中提取说话人的嘴唇图像。使用嘴唇界标点提取以嘴唇中心为中心的宽高比为$1:2$的区域，将提取的区域调整为$40$像素$\times80$像素的大小并转换为灰度图像。应当注意，以$25\text{ fps}$提取嘴唇序列，并且以$75\text{ VPS}$提取音频特征。

### 5.3. Experimental Setup

对于AV特征融合和mask估计，使用TensorFlow库和NVIDIA Titan Xp GPU对网络进行了训练。Grid ChiME 3语料库的一部分说话者(如第5.1节所述) 用于神经网络的训练/验证，其余说话者用于在说话者无关的情况下测试经过训练的神经网络的性能($25％$测试数据集) 。Grid ChiME 3语料库的预处理训练集由大约25000句话组成，分为21000和4000句话分别用于训练和验证。要注意的是，在训练，验证和测试集合中存在的说话人和噪声之间没有重叠，以确保说话人和噪声独立标准。当遇到丢失的视觉帧时，将使用零向量代替嘴唇图像。预处理的数据集由裁剪后的嘴唇图像和嘈杂的音频频谱图作为输入，而IBM作为输出。通过使用Adam优化器[38]进行反向传播对网络进行训练，直到验证错误停止减小。

### 5.4. Objective testing on Synthetic mixtures

使用以下评估语音清晰度的客观指标和上述合成AV数据集评估重新合成的语音的质量(第5.1节)

#### 5.4.1. Perceptual Evaluation of Speech quality (PESQ) comparison

Table4：在Grid ChiME 3说话人独立测试集上，使用SEGAN [43]，SS [41]，LMMSE [42]，仅音频(A) CochleaNet，视听(AV) CochleaNet和理想IBM合成的语音来计算的的PESQ分数。包含用于未处理(带噪声) 信号的参考PESQ，以进行相对比较。

Table5：在TCD-TIMIT+MUSAN AV测试集上，使用SEGAN [43]，SS [41]，LMMSE [42]，仅音频(A) CochleaNet，视听(AV) CochleaNet和理想IBM合成的语音来计算的的PESQ分数。包含用于未处理(带噪声) 信号的参考PESQ，以进行相对比较。

Table6：在 Hou[15]+NOISEX92 AV测试集上，使用SEGAN [43]，SS [41]，LMMSE [42]，仅音频(A) CochleaNet，视听(AV) CochleaNet和理想IBM合成的语音来计算的的PESQ分数。包含用于未处理(带噪声) 信号的参考PESQ，以进行相对比较。

Figure5：在(a) Grid+ChiME3 (b) TCD+MUSAN (c) Hou等[15]+NOISEx-92 AV数据集上，使用SEGAN [43]，SS [41]，LMMSE [42]，纯音频(A) CochleaNet，视听(AV) CochleaNet和理想IBM重新合成的语音计算出的PESQ分数。包括未处理(带噪声的) 信号的参考PESQ，用于比较。

PESQ [39]是SE文献中最常用的客观评估指标之一，并且已证明与主观听觉测验密切相关[40]。PESQ计算为参考信号和修改信号之间的平均干扰值和平均非对称干扰值的线性组合。PESQ分数范围为[-0.50,4.50]，表示可能的最小和最大重构语音质量。表4、5、6分别列出了A-only和AV CochleaNet，SEGAN，SS和LMMSE在Grid+ChiME 3，TCD TIMIT+MUSAN和Hou等人[15]+NOISEX-92数据集上对于不同的SNR的PESQ得分。各种数据集确保说话者和噪声无关的标准，庞大的词汇语料库以及与语言无关的场景。要注意的是，在Grid ChiME 3语料库上训练的模型用于评估。可以看出，在低信噪比下，AV CochleaNet和仅A CochleaNet优于基于SS [41]，LMMSE [42]和SEGAN [43]的SE方法。此外，AV的性能比A-only CochleaNet更好，尤其是在低SNR范围(即$SNR\lt0\text{ dB}$) 时，在与Grid ChiME 3说话人无关的测试集上，在SNR分别为$-12\text{ dB}$，$-9\text{ dB}$和$-6\text{ dB}$时，AV CochleaNet模型达到了$1.97$，$2.16$和$2.33$的PESQ得分，而的仅A CochleaNet模型仅获得了$1.84$，$2.04$和$2.24$的PESQ得分。但是，在高SNR(即$SNR\gt0\text{ dB}$) 下，AV略胜于仅A的mask估计模型，在与Grid ChiME 3说话人无关的测试集上，在SNR分别为$0\text{ dB}$，$3\text{ dB}$和$6\text{ dB}$时，AV CochleaNet达到了$2.58$，$2.69$和$2.79$的PESQ分数而仅A的CochleaNet模型也获得了$2.52$，$2.63$和$2.73$。与嘈杂音频相比，PESQ的总体改进如图5所示，其中AV CochleaNet优于仅A的CochleaNet，并且为Grid ChiME 3语料库实现了接近最佳的性能(接近理想的IBM) 。

#### 5.4.2. Short Term Objective Intelligibility (STOI) comparison

Figure6：在(a) Grid+ChiME3 (b) TCD+MUSAN (c) Hou等[15]+NOISEx-92 AV数据集上，使用SEGAN [43]，SS [41]，LMMSE [42]，纯音频(A) CochleaNet，视听(AV) CochleaNet和理想IBM重新合成的语音计算出的STOI分数。包括未处理(带噪声的) 信号的参考STOI，用于比较。

STOI是另一种用于语音清晰度的基准客观评估指标，它与主观听力测试分数具有高度相关性[44]。在STOI中计算的干净语音和修饰语音之间的短时时间包络的相关性介于[0,1]范围内，较高的值表示更好的清晰度。图6显示了A-only和AV CochleaNet，SEGAN，SS和LMMSE在Grid+ChiME 3，TCD TIMIT+MUSAN和Hou等[15]+NOISEX-92数据集上，针对不同SNR的STOI得分。可以看出。在低信噪比下，AV CochleaNet和仅A CochleaNet的性能优于基于SS[41]，LMMSE[42]，SEGAN[43]的SE方法。此外，在低SNR范围$(SNR\lt0\text{ dB})$下，AV的性能要优于仅A模型，其中在Hou[15]+NOISEX-92语言独立的测试集上，SNR分别为$-12\text{ dB}$，$-9\text{ dB}$，$-6\text{ dB}$时，AV CochleaNet模型获得了$0.51$、$0.560$和$0.607$的STOI得分而仅A的CochleaNet模型获得了$0.483$，$0.513$和$0.544$。但是，在高SNR$(SNR\gt0\text{ dB})$下，AV略胜于仅使用A的 mask估计模型，其中在Hou[15]+NOISEX-92语言独立的测试集上，SNR分别为$0\text{ dB}$，$3\text{ dB}$，$6\text{ dB}$时，AV CochleaNet获得了$0.719$，$0.739$和$0.776$的STOI分数而仅A CochleaNet模型获得的$0.665$，$0.701$和$0.752$。

#### 5.4.3. Scale-Invariant Signal-to-Distortion Ratio (SI-SDR) comparison

Figure7：在(a) Grid+ChiME3 (b) TCD+MUSAN (c) Hou等[15]+NOISEx-92 AV数据集上，使用SEGAN [43]，SS [41]，LMMSE [42]，纯音频(A) CochleaNet，视听(AV) CochleaNet和理想IBM重新合成的语音计算出的SI-SDR分数。包括未处理(带噪声的) 信号的参考SI-SDR，用于比较。

Figure9：来自GRID+ChiME3说话者独立测试集的随机选的-6 dB的增强语音的频谱图。可以看出，仅A和AV CochleaNet的性能优于基于SS，LMMSE和SEGAN的增强功能。值得注意的是，AV CochleaNet比仅使用A的CochleaNet能够更好地恢复某些频率分量。

SI-SDR [45]是SDR的比例尺不变版本。SDR是测量分离信号引入的失真量的标准语音分离评估指标之一，并定义为干净信号能量与失真能量之间的比率。较高的SDR值表示更好的语音分离性能。图7分别显示了A-only和AV CochleaNet，SEGAN，SS和LMMSE在Grid+ChiME 3，TCD TIMIT+MUSAN和Hou等[15]+NOISEX-92数据集上对于不同的SNR的SI-SDR分数。可以看出，在低信噪比下，AV CochleaNet和仅A CochleaNet优于基于SS [41]，LMMSE [42]，SEGAN [43]的SE的方法。此外，在低SNR范围$(SNR\lt0\text{ dB})$下，AV的性能要优于仅A mask估计模型，其中在TED-TIMIT+MUSAN说话人独立和大量词汇测试集下，SNR分别为$-12\text{ dB}$，$-9\text{ dB}$和$-6\text{ dB}$时，AV CochleaNet模型获得了$3.62$, $4.80$和$5.41$的SI-SDR分数而A-only CochleaNet模型获得了$3.04$，$4.41$和$5.29$。但是，在高SNR$(SNR\gt0\text{ dB})$下，AV略胜于仅使用A的 mask估计模型，其中在TED-TIMIT+MUSAN说话人独立和大量词汇测试集下，SNR分别为$0\text{ dB}$，$3\text{ dB}$和$6\text{ dB}$时，AV CochleaNet达到了$7.77$，$8.64$和$9.31$的SI-SDR分数，而仅A的CochleaNet获得了$7.76$，$8.62$和$9.27$。

图9展示了从GRID+ChiME 3 AV语料库选出的随机一句话的带噪声的频谱图和使用SS，LMMSE，SEGAN +，仅A CochleaNet，AV CochleaNet和原始IBM的重构语音信号的频谱图。要注意的是，语音完全被背景噪声淹没，并且可以看到CochleaNet模型的性能(即接近Oracle IBM) 。

### 5.5. Subjective testing on ASPIRE Corpus

图8：使用SS [41]，LMMSE [42]，SEGAN [43]，仅A CochleaNet，AV CochleaNet对ASPIRE语料库的重构语音信号进行MUSHRA听力测试的结果。包括未处理(噪声) 信号的参考MUSHRA分数，用于相对比较。

在文献中，已经提出了大量客观指标[39、44、45]，以计算方式近似主观听力测试。然而，量化主观质量的唯一方法是向听众征求意见。我们使用MUSHRA风格[46]的听力测试方法进行主观评估，并使用来自真实嘈杂的ASPIRE语料库的增强语音(第3节) 。总共20名具有正常听力的英语母语者参加了听力测试。单个测试包括从ASPIRE语料库中随机抽取的20句话。前两个屏幕用于培训参与者，以调节音量并熟悉屏幕和任务。在每个屏幕中，要求参与者根据每个SE模型针对同一句子从[0,100]的等级对每个音频样本的质量进行评分。[80,100]的范围被描述为极好，而[60，80]是好，[40,60]是一般，[20,40]是差，[0,20]是极差。测试中包括了嘈杂的语音，以便参与者可以参考退化语音以及检查参与者是否通过了材料。

还记录了完成每个屏幕所需的时间，并将其用于消除任何异常值。我们评估了五个SE模型，包括：SEAGN，SS，LMMSE，仅A的CochleaNet和AV CochleaNet。图8显示了在ASPIRE语料库上根据系统排名的说话者响应的箱形图。听力测试结果表明，与基于A的CochleaNet，SEGAN，基于谱减法(SS) 和对数最小均方误差(LMMSE) 的SE方法相比，我们的AV CochleaNet具有更好的性能。结果证明了CochleaNet具有通过利用音频和视频提示来处理在现实环境中观察到的多个带混响竞争背景源的能力。此外，结果表明，在合成混合物上训练的AV模型可以很好地推广真实的嘈杂语料库。

### 5.6. Additional Analysis

**被遮挡的视觉信息的影响** 该模型在专业记录的语料库上进行了训练和评估，以确保所有视觉框都不包含被遮挡的嘴唇图像(少量的缺少视觉效果的Grid语料库话语除外) 。但是，在现实生活中，特别是当源和目标不稳定时，该模型需要针对丢失的视觉信息具有鲁棒性。因此，为了实验评估在这种情况下训练好的AV CochleaNet行为，我们用空白的视觉框架随机替换了一定比例的嘴唇图像。嘴唇闭塞的结果如图10所示。可以看出，对于-9 dB和-12 dB，随着视觉闭塞的增加，PESQ评分最初保持恒定，并且在20％的闭塞之后开始逐渐减小。值得一提的是，即使在训练过程中没有遇到这种情况，但完全没有视觉效果时，AV模型的性能与仅A模型相似。

Figure10：不同百分比的遮挡的嘴唇图像的PESQ分数

**纯音频和视听CochleaNet的音素级别比较** 文献中众所周知，视觉信息有助于消除语音逻辑上的歧义。此外，某些音素(例如/p/) 在视觉上是可区分的，而音素(例如/g/) 则无法在视觉上进行区分。但是，视觉上可分辨的音素与AV SE性能之间的关系尚不清楚。

因此，我们使用Grid+CHiME 3说话人独立测试集对3个听众和1000句随机增强语音进行了对比听觉测试，以凭经验确定在视觉上可分辨的音素与AV CochleaNet可以比仅使用A CochleaNet增强的音素之间是否存在关系。听力测试表明，AV模型增强了/r/，/p/，/l/，/w/，/EH1/，/AE1/，/IY1/，/EY1/，/AA1/和/OW1/音素而 /h/，/g/和/k/等音素的AV性能类似于纯音模型。这证实了这样的假设，即视觉上可辨别的音素与AV模型可以更好地使用的音素之间存在直接的关系。

**静音语音区域中纯音频和视听CochleaNet的比较** AV CochleaNet与仅A的CochleaNet相比的优越性能可能是由于视觉提示，特别是闭合的嘴唇可以为静音语音中的AV模型提供更多信息地区。为了验证这一假设，我们计算了无声语音区域中的预测mask与IBM之间的均方误差(MSE) 。与仅实现MSE为0.0108的AV相比，仅A模型实现了0.0123的MSE。这证实了上述假设，但是需要进一步分析以可视化卷积接受场，并在说话者保持沉默时检查模型的特定部分是否处于活动状态。图11给出了使用SS，LMMSE，SEGAN+，A-only CochleaNet，AV CochleaNet从TCD-TIMIT语料库中随机选的一句话的重构语音信号的噪声频谱图和频谱图。可以看出，语音被背景噪声完全淹没，并且与SS，LMMSE和SEGAN+相比，纯A和AV CochleaNet能够抑制噪声占主导的区域和语音占主导的区域。可以看出，在无声语音区域中，AV CochleaNet优于仅A的CochleaNet。

Figure11：来自ASPIRE语料库的随机的增强语音的频谱图。值得注意的是，AV CochleaNet优于A-only CochleaNet，特别是在视觉提示（嘴唇位置）有助于识别说话者是否在说话的无声语音区域中。

提出的工作的主要局限性在于：

1. 基于IBM SE的过程忽略了导致无效STFT问题的相位谱[20]
2. 如果多个讲话者同时讲话时，该模型无法分离重叠的讲话。该模型未使用这种混合AV语料库进行训练
3. ASPIRE语料库仅包含三个在受控的真实嘈杂环境中录制的说话者，并具有固定的说话者-听者设置，并且需要更具挑战性的非平稳真实噪音语料来评估模型的鲁棒性
4. 提出的模型仅适用于单声道音频，无法利用我们每天遇到的语音的双耳性质
5. 在听力设备(如助听器和人工耳蜗) 中部署提出的基于mask估计的模型的主要瓶颈是数据隐私问题，高处理能力要求和处理延迟

## 6. Conclusion

本文提出了一种针对SE的因果的，语言，噪声和说话者独立的AVDNN模型，该模型在上下文中利用了音频和视频提示，独立于SNR，以估计频谱IBM并增强语音。此外，我们提出了一种新颖的AV语料库ASPIRE1，它由在诸如餐厅和饭店等真实环境中录制的语音组成，以评估所提出的模型。语料库可以被语音社区用作资源来评估AV SE模型。我们进行了广泛的实验，同时考虑了噪音，说话者和语言无关的标准。在客观指标（PESQ，SI-SDR和ESTOI）和主观MUSHRA听力测试方面的性能评估表明，与仅使用A的CochleaNet，最先进的SE（包括SS，LMMSE）方法以及基于DNN的SE方法（包括SEGAN）。仿真结果验证了在低信噪比时视觉效果更有效，在高信噪比时视觉效果较差的现象。视觉遮挡研究表明，模型性能最初保持恒定，直到移除20％的视觉效果，而在遮挡20％之后，性能随着遮挡帧数的增加而线性降低。与仅使用A的模型相比，确定视觉提示在AV模型的优越性能中所起的作用的经验研究表明，在视觉上可分辨的音素与AV模型的性能之间存在高度相关性。此外，研究表明，AV模型在无声语音区域中的性能明显优于仅A语言，因为相比于仅使用音频输入，AV模式相对容易辨别说话者是否在说话。将来，我们打算研究我们提出的DNN模型与其他更具挑战性的会话性真实嘈杂的AV语料库的泛化能力。正在进行的未来工作还通过多模式AV助听器解决了实时实施方面的挑战和隐私问题。
